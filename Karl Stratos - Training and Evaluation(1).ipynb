{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3b62409-9c59-4b47-9bea-ab0c04fa0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import scipy.stats\n",
    "import math, string, re\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn_crfsuite import CRF\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.util import untag\n",
    "\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "\n",
    "import jieba \n",
    "from hanziconv import HanziConv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de406276-8495-4d3e-8545-b9da784fb105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2527469e-e780-4db1-a10c-9ba837969de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6318 entries, 0 to 6317\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tagged  6318 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 49.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# read in the seniors previous training data \n",
    "df = pd.read_csv(\"seniors_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7420aa6c-ac00-46ba-b78c-9dd18f06c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('In', 'ADP'), ('a', 'DET'), ('webinar', 'NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('I', 'PRON'), ('think', 'VERB'), ('that', 'D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[('Goooood', 'X'), (',', 'PUNCT'), ('very', 'A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[('!', 'PUNCT'), ('nice', 'ADJ'), ('to', 'ADP'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[('item', 'NOUN'), ('with', 'ADP'), ('nice', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tagged\n",
       "0  [('In', 'ADP'), ('a', 'DET'), ('webinar', 'NOU...\n",
       "1  [('I', 'PRON'), ('think', 'VERB'), ('that', 'D...\n",
       "2  [('Goooood', 'X'), (',', 'PUNCT'), ('very', 'A...\n",
       "3  [('!', 'PUNCT'), ('nice', 'ADJ'), ('to', 'ADP'...\n",
       "4  [('item', 'NOUN'), ('with', 'ADP'), ('nice', '..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afc400d6-dd17-49b2-8e5c-a27c545a0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string2_list(text):\n",
    "    return ast.literal_eval(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e34c0e4-7cfe-4aad-b502-add4dc7b4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    # \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    \n",
    "    feature_set =  {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'prefix-4': sentence[index][:4],\n",
    "        'prefix-5': sentence[index][:5],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'suffix-4': sentence[index][-4:],\n",
    "        'suffix-5': sentence[index][-5:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'prev2_word': '' if index == 0 else sentence[index - 2],\n",
    "        'next2_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  else sentence[index + 2],\n",
    "        'prev3_word': '' if index == 0 else sentence[index - 3],\n",
    "        'next3_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  or index == len(sentence) - 3  else sentence[index + 3],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "        'natural_number': (re.findall(r'^[0-9]+', sentence[index])),\n",
    "        'initcaps' : (re.findall(r'^[A-Z]\\w+', sentence[index])),\n",
    "        'initcapsalpha': (re.findall(r'^[A-Z][a-z]\\w+', sentence[index])),\n",
    "        'word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', sentence[index].lower()),\n",
    "        'word.ispunctuation': (sentence[index] in string.punctuation)\n",
    "    }\n",
    "    \n",
    "    if index <= 0:\n",
    "        feature_set['BOS'] = True\n",
    "    \n",
    "    if index > len(sentence)-1:\n",
    "        feature_set['EOS'] = True\n",
    "        \n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cca0b04-e0f3-4f7d-a907-aa70d1f1028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6485d896-e9a4-4e93-a034-ce725d9633be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence, model):\n",
    "    sentence = sentence_splitter(sentence)\n",
    "    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n",
    "    return list(zip(sentence, model.predict([sentence_features])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4af68e1d-5568-4c25-847b-707602cd1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_splitter(sentence):\n",
    "    result = []\n",
    "    sents = word_tokenize(sentence)\n",
    "    for s in sents:\n",
    "        if re.findall(r'[\\u4e00-\\u9fff]+', s):\n",
    "            s = HanziConv.toSimplified(s)\n",
    "            result = result + list(jieba.cut(s, cut_all=False))\n",
    "        else:\n",
    "            result.append(s)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfb4b7ca-0dab-4f1b-881c-6b48d93f2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for training and testing\n",
    "data = df.tagged.apply(convert_string2_list)\n",
    "data = data.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7181a641-c2ec-4273-8f1d-b933c71d7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(.80 * len(data))\n",
    "training_sentences = data[:cutoff]\n",
    "test_sentences = data[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ddee64c-f2af-417d-a9ea-521cf764870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b45394a-4e62-4591-929f-90a362a2cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))     \n",
    "print(len(X_test))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7135ae91-f4b4-415f-8e46-8506ca2a43e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'word': 'In',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'I',\n",
       "   'prefix-2': 'In',\n",
       "   'prefix-3': 'In',\n",
       "   'prefix-4': 'In',\n",
       "   'prefix-5': 'In',\n",
       "   'suffix-1': 'n',\n",
       "   'suffix-2': 'In',\n",
       "   'suffix-3': 'In',\n",
       "   'suffix-4': 'In',\n",
       "   'suffix-5': 'In',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'a',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'webinar',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'last',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['In'],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'in',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': 'a',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'a',\n",
       "   'prefix-2': 'a',\n",
       "   'prefix-3': 'a',\n",
       "   'prefix-4': 'a',\n",
       "   'prefix-5': 'a',\n",
       "   'suffix-1': 'a',\n",
       "   'suffix-2': 'a',\n",
       "   'suffix-3': 'a',\n",
       "   'suffix-4': 'a',\n",
       "   'suffix-5': 'a',\n",
       "   'prev_word': 'In',\n",
       "   'next_word': 'webinar',\n",
       "   'prev2_word': '.',\n",
       "   'next2_word': 'last',\n",
       "   'prev3_word': 'pandemic',\n",
       "   'next3_word': 'week',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'a',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'webinar',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'we',\n",
       "   'prefix-3': 'web',\n",
       "   'prefix-4': 'webi',\n",
       "   'prefix-5': 'webin',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'ar',\n",
       "   'suffix-3': 'nar',\n",
       "   'suffix-4': 'inar',\n",
       "   'suffix-5': 'binar',\n",
       "   'prev_word': 'a',\n",
       "   'next_word': 'last',\n",
       "   'prev2_word': 'In',\n",
       "   'next2_word': 'week',\n",
       "   'prev3_word': '.',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'webinar',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'last',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'l',\n",
       "   'prefix-2': 'la',\n",
       "   'prefix-3': 'las',\n",
       "   'prefix-4': 'last',\n",
       "   'prefix-5': 'last',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'st',\n",
       "   'suffix-3': 'ast',\n",
       "   'suffix-4': 'last',\n",
       "   'suffix-5': 'last',\n",
       "   'prev_word': 'webinar',\n",
       "   'next_word': 'week',\n",
       "   'prev2_word': 'a',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': 'In',\n",
       "   'next3_word': 'Capital',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'last',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'week',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'we',\n",
       "   'prefix-3': 'wee',\n",
       "   'prefix-4': 'week',\n",
       "   'prefix-5': 'week',\n",
       "   'suffix-1': 'k',\n",
       "   'suffix-2': 'ek',\n",
       "   'suffix-3': 'eek',\n",
       "   'suffix-4': 'week',\n",
       "   'suffix-5': 'week',\n",
       "   'prev_word': 'last',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'webinar',\n",
       "   'next2_word': 'Capital',\n",
       "   'prev3_word': 'a',\n",
       "   'next3_word': 'Economics',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'week',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'week',\n",
       "   'next_word': 'Capital',\n",
       "   'prev2_word': 'last',\n",
       "   'next2_word': 'Economics',\n",
       "   'prev3_word': 'webinar',\n",
       "   'next3_word': \"\\\\'\",\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'Capital',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'C',\n",
       "   'prefix-2': 'Ca',\n",
       "   'prefix-3': 'Cap',\n",
       "   'prefix-4': 'Capi',\n",
       "   'prefix-5': 'Capit',\n",
       "   'suffix-1': 'l',\n",
       "   'suffix-2': 'al',\n",
       "   'suffix-3': 'tal',\n",
       "   'suffix-4': 'ital',\n",
       "   'suffix-5': 'pital',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'Economics',\n",
       "   'prev2_word': 'week',\n",
       "   'next2_word': \"\\\\'\",\n",
       "   'prev3_word': 'last',\n",
       "   'next3_word': 'global',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Capital'],\n",
       "   'initcapsalpha': ['Capital'],\n",
       "   'word.stemmed': 'capital',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'Economics',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'E',\n",
       "   'prefix-2': 'Ec',\n",
       "   'prefix-3': 'Eco',\n",
       "   'prefix-4': 'Econ',\n",
       "   'prefix-5': 'Econo',\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': 'cs',\n",
       "   'suffix-3': 'ics',\n",
       "   'suffix-4': 'mics',\n",
       "   'suffix-5': 'omics',\n",
       "   'prev_word': 'Capital',\n",
       "   'next_word': \"\\\\'\",\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': 'global',\n",
       "   'prev3_word': 'week',\n",
       "   'next3_word': 'economists',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Economics'],\n",
       "   'initcapsalpha': ['Economics'],\n",
       "   'word.stemmed': 'economics',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': \"\\\\'\",\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '\\\\',\n",
       "   'prefix-2': \"\\\\'\",\n",
       "   'prefix-3': \"\\\\'\",\n",
       "   'prefix-4': \"\\\\'\",\n",
       "   'prefix-5': \"\\\\'\",\n",
       "   'suffix-1': \"'\",\n",
       "   'suffix-2': \"\\\\'\",\n",
       "   'suffix-3': \"\\\\'\",\n",
       "   'suffix-4': \"\\\\'\",\n",
       "   'suffix-5': \"\\\\'\",\n",
       "   'prev_word': 'Economics',\n",
       "   'next_word': 'global',\n",
       "   'prev2_word': 'Capital',\n",
       "   'next2_word': 'economists',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'lumped',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': \"\\\\'\",\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'global',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'g',\n",
       "   'prefix-2': 'gl',\n",
       "   'prefix-3': 'glo',\n",
       "   'prefix-4': 'glob',\n",
       "   'prefix-5': 'globa',\n",
       "   'suffix-1': 'l',\n",
       "   'suffix-2': 'al',\n",
       "   'suffix-3': 'bal',\n",
       "   'suffix-4': 'obal',\n",
       "   'suffix-5': 'lobal',\n",
       "   'prev_word': \"\\\\'\",\n",
       "   'next_word': 'economists',\n",
       "   'prev2_word': 'Economics',\n",
       "   'next2_word': 'lumped',\n",
       "   'prev3_word': 'Capital',\n",
       "   'next3_word': 'together',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'global',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'economists',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'e',\n",
       "   'prefix-2': 'ec',\n",
       "   'prefix-3': 'eco',\n",
       "   'prefix-4': 'econ',\n",
       "   'prefix-5': 'econo',\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': 'ts',\n",
       "   'suffix-3': 'sts',\n",
       "   'suffix-4': 'ists',\n",
       "   'suffix-5': 'mists',\n",
       "   'prev_word': 'global',\n",
       "   'next_word': 'lumped',\n",
       "   'prev2_word': \"\\\\'\",\n",
       "   'next2_word': 'together',\n",
       "   'prev3_word': 'Economics',\n",
       "   'next3_word': 'the',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'economists',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'lumped',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'l',\n",
       "   'prefix-2': 'lu',\n",
       "   'prefix-3': 'lum',\n",
       "   'prefix-4': 'lump',\n",
       "   'prefix-5': 'lumpe',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'ed',\n",
       "   'suffix-3': 'ped',\n",
       "   'suffix-4': 'mped',\n",
       "   'suffix-5': 'umped',\n",
       "   'prev_word': 'economists',\n",
       "   'next_word': 'together',\n",
       "   'prev2_word': 'global',\n",
       "   'next2_word': 'the',\n",
       "   'prev3_word': \"\\\\'\",\n",
       "   'next3_word': 'Philippines',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'lumped',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'together',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'to',\n",
       "   'prefix-3': 'tog',\n",
       "   'prefix-4': 'toge',\n",
       "   'prefix-5': 'toget',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'er',\n",
       "   'suffix-3': 'her',\n",
       "   'suffix-4': 'ther',\n",
       "   'suffix-5': 'ether',\n",
       "   'prev_word': 'lumped',\n",
       "   'next_word': 'the',\n",
       "   'prev2_word': 'economists',\n",
       "   'next2_word': 'Philippines',\n",
       "   'prev3_word': 'global',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'together',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'the',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'th',\n",
       "   'prefix-3': 'the',\n",
       "   'prefix-4': 'the',\n",
       "   'prefix-5': 'the',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'he',\n",
       "   'suffix-3': 'the',\n",
       "   'suffix-4': 'the',\n",
       "   'suffix-5': 'the',\n",
       "   'prev_word': 'together',\n",
       "   'next_word': 'Philippines',\n",
       "   'prev2_word': 'lumped',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': 'economists',\n",
       "   'next3_word': 'Thailand',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'th',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'Philippines',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'P',\n",
       "   'prefix-2': 'Ph',\n",
       "   'prefix-3': 'Phi',\n",
       "   'prefix-4': 'Phil',\n",
       "   'prefix-5': 'Phili',\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': 'es',\n",
       "   'suffix-3': 'nes',\n",
       "   'suffix-4': 'ines',\n",
       "   'suffix-5': 'pines',\n",
       "   'prev_word': 'the',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'together',\n",
       "   'next2_word': 'Thailand',\n",
       "   'prev3_word': 'lumped',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Philippines'],\n",
       "   'initcapsalpha': ['Philippines'],\n",
       "   'word.stemmed': 'philippines',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'Philippines',\n",
       "   'next_word': 'Thailand',\n",
       "   'prev2_word': 'the',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': 'together',\n",
       "   'next3_word': 'Mexico',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'Thailand',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'T',\n",
       "   'prefix-2': 'Th',\n",
       "   'prefix-3': 'Tha',\n",
       "   'prefix-4': 'Thai',\n",
       "   'prefix-5': 'Thail',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'nd',\n",
       "   'suffix-3': 'and',\n",
       "   'suffix-4': 'land',\n",
       "   'suffix-5': 'iland',\n",
       "   'prev_word': ',',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'Philippines',\n",
       "   'next2_word': 'Mexico',\n",
       "   'prev3_word': 'the',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Thailand'],\n",
       "   'initcapsalpha': ['Thailand'],\n",
       "   'word.stemmed': 'thailand',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'Thailand',\n",
       "   'next_word': 'Mexico',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': 'Philippines',\n",
       "   'next3_word': 'and',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'Mexico',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'M',\n",
       "   'prefix-2': 'Me',\n",
       "   'prefix-3': 'Mex',\n",
       "   'prefix-4': 'Mexi',\n",
       "   'prefix-5': 'Mexic',\n",
       "   'suffix-1': 'o',\n",
       "   'suffix-2': 'co',\n",
       "   'suffix-3': 'ico',\n",
       "   'suffix-4': 'xico',\n",
       "   'suffix-5': 'exico',\n",
       "   'prev_word': ',',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'Thailand',\n",
       "   'next2_word': 'and',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'Southern',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Mexico'],\n",
       "   'initcapsalpha': ['Mexico'],\n",
       "   'word.stemmed': 'mexic',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'Mexico',\n",
       "   'next_word': 'and',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': 'Southern',\n",
       "   'prev3_word': 'Thailand',\n",
       "   'next3_word': 'Europe',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'and',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'a',\n",
       "   'prefix-2': 'an',\n",
       "   'prefix-3': 'and',\n",
       "   'prefix-4': 'and',\n",
       "   'prefix-5': 'and',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'nd',\n",
       "   'suffix-3': 'and',\n",
       "   'suffix-4': 'and',\n",
       "   'suffix-5': 'and',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'Southern',\n",
       "   'prev2_word': 'Mexico',\n",
       "   'next2_word': 'Europe',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'among',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'and',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'Southern',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'S',\n",
       "   'prefix-2': 'So',\n",
       "   'prefix-3': 'Sou',\n",
       "   'prefix-4': 'Sout',\n",
       "   'prefix-5': 'South',\n",
       "   'suffix-1': 'n',\n",
       "   'suffix-2': 'rn',\n",
       "   'suffix-3': 'ern',\n",
       "   'suffix-4': 'hern',\n",
       "   'suffix-5': 'thern',\n",
       "   'prev_word': 'and',\n",
       "   'next_word': 'Europe',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': 'among',\n",
       "   'prev3_word': 'Mexico',\n",
       "   'next3_word': 'the',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Southern'],\n",
       "   'initcapsalpha': ['Southern'],\n",
       "   'word.stemmed': 'souther',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'Europe',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'E',\n",
       "   'prefix-2': 'Eu',\n",
       "   'prefix-3': 'Eur',\n",
       "   'prefix-4': 'Euro',\n",
       "   'prefix-5': 'Europ',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'pe',\n",
       "   'suffix-3': 'ope',\n",
       "   'suffix-4': 'rope',\n",
       "   'suffix-5': 'urope',\n",
       "   'prev_word': 'Southern',\n",
       "   'next_word': 'among',\n",
       "   'prev2_word': 'and',\n",
       "   'next2_word': 'the',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'economies',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Europe'],\n",
       "   'initcapsalpha': ['Europe'],\n",
       "   'word.stemmed': 'europ',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'among',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'a',\n",
       "   'prefix-2': 'am',\n",
       "   'prefix-3': 'amo',\n",
       "   'prefix-4': 'amon',\n",
       "   'prefix-5': 'among',\n",
       "   'suffix-1': 'g',\n",
       "   'suffix-2': 'ng',\n",
       "   'suffix-3': 'ong',\n",
       "   'suffix-4': 'mong',\n",
       "   'suffix-5': 'among',\n",
       "   'prev_word': 'Europe',\n",
       "   'next_word': 'the',\n",
       "   'prev2_word': 'Southern',\n",
       "   'next2_word': 'economies',\n",
       "   'prev3_word': 'and',\n",
       "   'next3_word': 'which',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'am',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'the',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'th',\n",
       "   'prefix-3': 'the',\n",
       "   'prefix-4': 'the',\n",
       "   'prefix-5': 'the',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'he',\n",
       "   'suffix-3': 'the',\n",
       "   'suffix-4': 'the',\n",
       "   'suffix-5': 'the',\n",
       "   'prev_word': 'among',\n",
       "   'next_word': 'economies',\n",
       "   'prev2_word': 'Europe',\n",
       "   'next2_word': 'which',\n",
       "   'prev3_word': 'Southern',\n",
       "   'next3_word': 'would',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'th',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'economies',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'e',\n",
       "   'prefix-2': 'ec',\n",
       "   'prefix-3': 'eco',\n",
       "   'prefix-4': 'econ',\n",
       "   'prefix-5': 'econo',\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': 'es',\n",
       "   'suffix-3': 'ies',\n",
       "   'suffix-4': 'mies',\n",
       "   'suffix-5': 'omies',\n",
       "   'prev_word': 'the',\n",
       "   'next_word': 'which',\n",
       "   'prev2_word': 'among',\n",
       "   'next2_word': 'would',\n",
       "   'prev3_word': 'Europe',\n",
       "   'next3_word': 'most',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'economies',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'which',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'wh',\n",
       "   'prefix-3': 'whi',\n",
       "   'prefix-4': 'whic',\n",
       "   'prefix-5': 'which',\n",
       "   'suffix-1': 'h',\n",
       "   'suffix-2': 'ch',\n",
       "   'suffix-3': 'ich',\n",
       "   'suffix-4': 'hich',\n",
       "   'suffix-5': 'which',\n",
       "   'prev_word': 'economies',\n",
       "   'next_word': 'would',\n",
       "   'prev2_word': 'the',\n",
       "   'next2_word': 'most',\n",
       "   'prev3_word': 'among',\n",
       "   'next3_word': 'likely',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'which',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'would',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'wo',\n",
       "   'prefix-3': 'wou',\n",
       "   'prefix-4': 'woul',\n",
       "   'prefix-5': 'would',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'ld',\n",
       "   'suffix-3': 'uld',\n",
       "   'suffix-4': 'ould',\n",
       "   'suffix-5': 'would',\n",
       "   'prev_word': 'which',\n",
       "   'next_word': 'most',\n",
       "   'prev2_word': 'economies',\n",
       "   'next2_word': 'likely',\n",
       "   'prev3_word': 'the',\n",
       "   'next3_word': 'experience',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'would',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'most',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'm',\n",
       "   'prefix-2': 'mo',\n",
       "   'prefix-3': 'mos',\n",
       "   'prefix-4': 'most',\n",
       "   'prefix-5': 'most',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'st',\n",
       "   'suffix-3': 'ost',\n",
       "   'suffix-4': 'most',\n",
       "   'suffix-5': 'most',\n",
       "   'prev_word': 'would',\n",
       "   'next_word': 'likely',\n",
       "   'prev2_word': 'which',\n",
       "   'next2_word': 'experience',\n",
       "   'prev3_word': 'economies',\n",
       "   'next3_word': 'permanent',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'most',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'likely',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'l',\n",
       "   'prefix-2': 'li',\n",
       "   'prefix-3': 'lik',\n",
       "   'prefix-4': 'like',\n",
       "   'prefix-5': 'likel',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'ly',\n",
       "   'suffix-3': 'ely',\n",
       "   'suffix-4': 'kely',\n",
       "   'suffix-5': 'ikely',\n",
       "   'prev_word': 'most',\n",
       "   'next_word': 'experience',\n",
       "   'prev2_word': 'would',\n",
       "   'next2_word': 'permanent',\n",
       "   'prev3_word': 'which',\n",
       "   'next3_word': 'loss',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'likel',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'experience',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'e',\n",
       "   'prefix-2': 'ex',\n",
       "   'prefix-3': 'exp',\n",
       "   'prefix-4': 'expe',\n",
       "   'prefix-5': 'exper',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ce',\n",
       "   'suffix-3': 'nce',\n",
       "   'suffix-4': 'ence',\n",
       "   'suffix-5': 'ience',\n",
       "   'prev_word': 'likely',\n",
       "   'next_word': 'permanent',\n",
       "   'prev2_word': 'most',\n",
       "   'next2_word': 'loss',\n",
       "   'prev3_word': 'would',\n",
       "   'next3_word': 'of',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'experienc',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'permanent',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'p',\n",
       "   'prefix-2': 'pe',\n",
       "   'prefix-3': 'per',\n",
       "   'prefix-4': 'perm',\n",
       "   'prefix-5': 'perma',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'nt',\n",
       "   'suffix-3': 'ent',\n",
       "   'suffix-4': 'nent',\n",
       "   'suffix-5': 'anent',\n",
       "   'prev_word': 'experience',\n",
       "   'next_word': 'loss',\n",
       "   'prev2_word': 'likely',\n",
       "   'next2_word': 'of',\n",
       "   'prev3_word': 'most',\n",
       "   'next3_word': 'output',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'permanent',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'loss',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'l',\n",
       "   'prefix-2': 'lo',\n",
       "   'prefix-3': 'los',\n",
       "   'prefix-4': 'loss',\n",
       "   'prefix-5': 'loss',\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': 'ss',\n",
       "   'suffix-3': 'oss',\n",
       "   'suffix-4': 'loss',\n",
       "   'suffix-5': 'loss',\n",
       "   'prev_word': 'permanent',\n",
       "   'next_word': 'of',\n",
       "   'prev2_word': 'experience',\n",
       "   'next2_word': 'output',\n",
       "   'prev3_word': 'likely',\n",
       "   'next3_word': 'from',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'loss',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'of',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'o',\n",
       "   'prefix-2': 'of',\n",
       "   'prefix-3': 'of',\n",
       "   'prefix-4': 'of',\n",
       "   'prefix-5': 'of',\n",
       "   'suffix-1': 'f',\n",
       "   'suffix-2': 'of',\n",
       "   'suffix-3': 'of',\n",
       "   'suffix-4': 'of',\n",
       "   'suffix-5': 'of',\n",
       "   'prev_word': 'loss',\n",
       "   'next_word': 'output',\n",
       "   'prev2_word': 'permanent',\n",
       "   'next2_word': 'from',\n",
       "   'prev3_word': 'experience',\n",
       "   'next3_word': 'their',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'of',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'output',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'o',\n",
       "   'prefix-2': 'ou',\n",
       "   'prefix-3': 'out',\n",
       "   'prefix-4': 'outp',\n",
       "   'prefix-5': 'outpu',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ut',\n",
       "   'suffix-3': 'put',\n",
       "   'suffix-4': 'tput',\n",
       "   'suffix-5': 'utput',\n",
       "   'prev_word': 'of',\n",
       "   'next_word': 'from',\n",
       "   'prev2_word': 'loss',\n",
       "   'next2_word': 'their',\n",
       "   'prev3_word': 'permanent',\n",
       "   'next3_word': 'respective',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'output',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'from',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'f',\n",
       "   'prefix-2': 'fr',\n",
       "   'prefix-3': 'fro',\n",
       "   'prefix-4': 'from',\n",
       "   'prefix-5': 'from',\n",
       "   'suffix-1': 'm',\n",
       "   'suffix-2': 'om',\n",
       "   'suffix-3': 'rom',\n",
       "   'suffix-4': 'from',\n",
       "   'suffix-5': 'from',\n",
       "   'prev_word': 'output',\n",
       "   'next_word': 'their',\n",
       "   'prev2_word': 'of',\n",
       "   'next2_word': 'respective',\n",
       "   'prev3_word': 'loss',\n",
       "   'next3_word': 'domestic',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'from',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'their',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'th',\n",
       "   'prefix-3': 'the',\n",
       "   'prefix-4': 'thei',\n",
       "   'prefix-5': 'their',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'ir',\n",
       "   'suffix-3': 'eir',\n",
       "   'suffix-4': 'heir',\n",
       "   'suffix-5': 'their',\n",
       "   'prev_word': 'from',\n",
       "   'next_word': 'respective',\n",
       "   'prev2_word': 'output',\n",
       "   'next2_word': 'domestic',\n",
       "   'prev3_word': 'of',\n",
       "   'next3_word': 'tourism',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'their',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'respective',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'r',\n",
       "   'prefix-2': 're',\n",
       "   'prefix-3': 'res',\n",
       "   'prefix-4': 'resp',\n",
       "   'prefix-5': 'respe',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 've',\n",
       "   'suffix-3': 'ive',\n",
       "   'suffix-4': 'tive',\n",
       "   'suffix-5': 'ctive',\n",
       "   'prev_word': 'their',\n",
       "   'next_word': 'domestic',\n",
       "   'prev2_word': 'from',\n",
       "   'next2_word': 'tourism',\n",
       "   'prev3_word': 'output',\n",
       "   'next3_word': 'sectors',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'respectiv',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'domestic',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'd',\n",
       "   'prefix-2': 'do',\n",
       "   'prefix-3': 'dom',\n",
       "   'prefix-4': 'dome',\n",
       "   'prefix-5': 'domes',\n",
       "   'suffix-1': 'c',\n",
       "   'suffix-2': 'ic',\n",
       "   'suffix-3': 'tic',\n",
       "   'suffix-4': 'stic',\n",
       "   'suffix-5': 'estic',\n",
       "   'prev_word': 'respective',\n",
       "   'next_word': 'tourism',\n",
       "   'prev2_word': 'their',\n",
       "   'next2_word': 'sectors',\n",
       "   'prev3_word': 'from',\n",
       "   'next3_word': 'due',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'domestic',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'tourism',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'to',\n",
       "   'prefix-3': 'tou',\n",
       "   'prefix-4': 'tour',\n",
       "   'prefix-5': 'touri',\n",
       "   'suffix-1': 'm',\n",
       "   'suffix-2': 'sm',\n",
       "   'suffix-3': 'ism',\n",
       "   'suffix-4': 'rism',\n",
       "   'suffix-5': 'urism',\n",
       "   'prev_word': 'domestic',\n",
       "   'next_word': 'sectors',\n",
       "   'prev2_word': 'respective',\n",
       "   'next2_word': 'due',\n",
       "   'prev3_word': 'their',\n",
       "   'next3_word': 'to',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'tourism',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'sectors',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'se',\n",
       "   'prefix-3': 'sec',\n",
       "   'prefix-4': 'sect',\n",
       "   'prefix-5': 'secto',\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': 'rs',\n",
       "   'suffix-3': 'ors',\n",
       "   'suffix-4': 'tors',\n",
       "   'suffix-5': 'ctors',\n",
       "   'prev_word': 'tourism',\n",
       "   'next_word': 'due',\n",
       "   'prev2_word': 'domestic',\n",
       "   'next2_word': 'to',\n",
       "   'prev3_word': 'respective',\n",
       "   'next3_word': 'the',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'sectors',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'due',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'd',\n",
       "   'prefix-2': 'du',\n",
       "   'prefix-3': 'due',\n",
       "   'prefix-4': 'due',\n",
       "   'prefix-5': 'due',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ue',\n",
       "   'suffix-3': 'due',\n",
       "   'suffix-4': 'due',\n",
       "   'suffix-5': 'due',\n",
       "   'prev_word': 'sectors',\n",
       "   'next_word': 'to',\n",
       "   'prev2_word': 'tourism',\n",
       "   'next2_word': 'the',\n",
       "   'prev3_word': 'domestic',\n",
       "   'next3_word': 'pandemic',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'du',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'to',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'to',\n",
       "   'prefix-3': 'to',\n",
       "   'prefix-4': 'to',\n",
       "   'prefix-5': 'to',\n",
       "   'suffix-1': 'o',\n",
       "   'suffix-2': 'to',\n",
       "   'suffix-3': 'to',\n",
       "   'suffix-4': 'to',\n",
       "   'suffix-5': 'to',\n",
       "   'prev_word': 'due',\n",
       "   'next_word': 'the',\n",
       "   'prev2_word': 'sectors',\n",
       "   'next2_word': 'pandemic',\n",
       "   'prev3_word': 'tourism',\n",
       "   'next3_word': '.',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'to',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'the',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'th',\n",
       "   'prefix-3': 'the',\n",
       "   'prefix-4': 'the',\n",
       "   'prefix-5': 'the',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'he',\n",
       "   'suffix-3': 'the',\n",
       "   'suffix-4': 'the',\n",
       "   'suffix-5': 'the',\n",
       "   'prev_word': 'to',\n",
       "   'next_word': 'pandemic',\n",
       "   'prev2_word': 'due',\n",
       "   'next2_word': '.',\n",
       "   'prev3_word': 'sectors',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'th',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'pandemic',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'p',\n",
       "   'prefix-2': 'pa',\n",
       "   'prefix-3': 'pan',\n",
       "   'prefix-4': 'pand',\n",
       "   'prefix-5': 'pande',\n",
       "   'suffix-1': 'c',\n",
       "   'suffix-2': 'ic',\n",
       "   'suffix-3': 'mic',\n",
       "   'suffix-4': 'emic',\n",
       "   'suffix-5': 'demic',\n",
       "   'prev_word': 'the',\n",
       "   'next_word': '.',\n",
       "   'prev2_word': 'to',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'due',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'pandemic',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': '.',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '.',\n",
       "   'prefix-2': '.',\n",
       "   'prefix-3': '.',\n",
       "   'prefix-4': '.',\n",
       "   'prefix-5': '.',\n",
       "   'suffix-1': '.',\n",
       "   'suffix-2': '.',\n",
       "   'suffix-3': '.',\n",
       "   'suffix-4': '.',\n",
       "   'suffix-5': '.',\n",
       "   'prev_word': 'pandemic',\n",
       "   'next_word': '',\n",
       "   'prev2_word': 'the',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'to',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '.',\n",
       "   'word.ispunctuation': True}],\n",
       " [{'word': 'I',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'I',\n",
       "   'prefix-2': 'I',\n",
       "   'prefix-3': 'I',\n",
       "   'prefix-4': 'I',\n",
       "   'prefix-5': 'I',\n",
       "   'suffix-1': 'I',\n",
       "   'suffix-2': 'I',\n",
       "   'suffix-3': 'I',\n",
       "   'suffix-4': 'I',\n",
       "   'suffix-5': 'I',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'think',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'that',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': \"\\\\'s\",\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'i',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': 'think',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'th',\n",
       "   'prefix-3': 'thi',\n",
       "   'prefix-4': 'thin',\n",
       "   'prefix-5': 'think',\n",
       "   'suffix-1': 'k',\n",
       "   'suffix-2': 'nk',\n",
       "   'suffix-3': 'ink',\n",
       "   'suffix-4': 'hink',\n",
       "   'suffix-5': 'think',\n",
       "   'prev_word': 'I',\n",
       "   'next_word': 'that',\n",
       "   'prev2_word': 'account',\n",
       "   'next2_word': \"\\\\'s\",\n",
       "   'prev3_word': 'therapy',\n",
       "   'next3_word': 'not',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'think',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'that',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'th',\n",
       "   'prefix-3': 'tha',\n",
       "   'prefix-4': 'that',\n",
       "   'prefix-5': 'that',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'at',\n",
       "   'suffix-3': 'hat',\n",
       "   'suffix-4': 'that',\n",
       "   'suffix-5': 'that',\n",
       "   'prev_word': 'think',\n",
       "   'next_word': \"\\\\'s\",\n",
       "   'prev2_word': 'I',\n",
       "   'next2_word': 'not',\n",
       "   'prev3_word': 'account',\n",
       "   'next3_word': 'an',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'that',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': \"\\\\'s\",\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '\\\\',\n",
       "   'prefix-2': \"\\\\'\",\n",
       "   'prefix-3': \"\\\\'s\",\n",
       "   'prefix-4': \"\\\\'s\",\n",
       "   'prefix-5': \"\\\\'s\",\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': \"'s\",\n",
       "   'suffix-3': \"\\\\'s\",\n",
       "   'suffix-4': \"\\\\'s\",\n",
       "   'suffix-5': \"\\\\'s\",\n",
       "   'prev_word': 'that',\n",
       "   'next_word': 'not',\n",
       "   'prev2_word': 'think',\n",
       "   'next2_word': 'an',\n",
       "   'prev3_word': 'I',\n",
       "   'next3_word': 'official',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': \"\\\\'s\",\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'not',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'n',\n",
       "   'prefix-2': 'no',\n",
       "   'prefix-3': 'not',\n",
       "   'prefix-4': 'not',\n",
       "   'prefix-5': 'not',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ot',\n",
       "   'suffix-3': 'not',\n",
       "   'suffix-4': 'not',\n",
       "   'suffix-5': 'not',\n",
       "   'prev_word': \"\\\\'s\",\n",
       "   'next_word': 'an',\n",
       "   'prev2_word': 'that',\n",
       "   'next2_word': 'official',\n",
       "   'prev3_word': 'think',\n",
       "   'next3_word': 'unbox',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'not',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'an',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'a',\n",
       "   'prefix-2': 'an',\n",
       "   'prefix-3': 'an',\n",
       "   'prefix-4': 'an',\n",
       "   'prefix-5': 'an',\n",
       "   'suffix-1': 'n',\n",
       "   'suffix-2': 'an',\n",
       "   'suffix-3': 'an',\n",
       "   'suffix-4': 'an',\n",
       "   'suffix-5': 'an',\n",
       "   'prev_word': 'not',\n",
       "   'next_word': 'official',\n",
       "   'prev2_word': \"\\\\'s\",\n",
       "   'next2_word': 'unbox',\n",
       "   'prev3_word': 'that',\n",
       "   'next3_word': 'therapy',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'an',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'official',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'o',\n",
       "   'prefix-2': 'of',\n",
       "   'prefix-3': 'off',\n",
       "   'prefix-4': 'offi',\n",
       "   'prefix-5': 'offic',\n",
       "   'suffix-1': 'l',\n",
       "   'suffix-2': 'al',\n",
       "   'suffix-3': 'ial',\n",
       "   'suffix-4': 'cial',\n",
       "   'suffix-5': 'icial',\n",
       "   'prev_word': 'an',\n",
       "   'next_word': 'unbox',\n",
       "   'prev2_word': 'not',\n",
       "   'next2_word': 'therapy',\n",
       "   'prev3_word': \"\\\\'s\",\n",
       "   'next3_word': 'account',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'official',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'unbox',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'u',\n",
       "   'prefix-2': 'un',\n",
       "   'prefix-3': 'unb',\n",
       "   'prefix-4': 'unbo',\n",
       "   'prefix-5': 'unbox',\n",
       "   'suffix-1': 'x',\n",
       "   'suffix-2': 'ox',\n",
       "   'suffix-3': 'box',\n",
       "   'suffix-4': 'nbox',\n",
       "   'suffix-5': 'unbox',\n",
       "   'prev_word': 'official',\n",
       "   'next_word': 'therapy',\n",
       "   'prev2_word': 'an',\n",
       "   'next2_word': 'account',\n",
       "   'prev3_word': 'not',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'unbox',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'therapy',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'th',\n",
       "   'prefix-3': 'the',\n",
       "   'prefix-4': 'ther',\n",
       "   'prefix-5': 'thera',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'py',\n",
       "   'suffix-3': 'apy',\n",
       "   'suffix-4': 'rapy',\n",
       "   'suffix-5': 'erapy',\n",
       "   'prev_word': 'unbox',\n",
       "   'next_word': 'account',\n",
       "   'prev2_word': 'official',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'an',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'therap',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'account',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'a',\n",
       "   'prefix-2': 'ac',\n",
       "   'prefix-3': 'acc',\n",
       "   'prefix-4': 'acco',\n",
       "   'prefix-5': 'accou',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'nt',\n",
       "   'suffix-3': 'unt',\n",
       "   'suffix-4': 'ount',\n",
       "   'suffix-5': 'count',\n",
       "   'prev_word': 'therapy',\n",
       "   'next_word': '',\n",
       "   'prev2_word': 'unbox',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'official',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'account',\n",
       "   'word.ispunctuation': False}],\n",
       " [{'word': 'Goooood',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'G',\n",
       "   'prefix-2': 'Go',\n",
       "   'prefix-3': 'Goo',\n",
       "   'prefix-4': 'Gooo',\n",
       "   'prefix-5': 'Goooo',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'od',\n",
       "   'suffix-3': 'ood',\n",
       "   'suffix-4': 'oood',\n",
       "   'suffix-5': 'ooood',\n",
       "   'prev_word': '',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'very',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'soft',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Goooood'],\n",
       "   'initcapsalpha': ['Goooood'],\n",
       "   'word.stemmed': 'goooood',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'Goooood',\n",
       "   'next_word': 'very',\n",
       "   'prev2_word': 'soft',\n",
       "   'next2_word': 'soft',\n",
       "   'prev3_word': 'very',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'very',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'v',\n",
       "   'prefix-2': 've',\n",
       "   'prefix-3': 'ver',\n",
       "   'prefix-4': 'very',\n",
       "   'prefix-5': 'very',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'ry',\n",
       "   'suffix-3': 'ery',\n",
       "   'suffix-4': 'very',\n",
       "   'suffix-5': 'very',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'soft',\n",
       "   'prev2_word': 'Goooood',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'soft',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'ver',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'soft',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'so',\n",
       "   'prefix-3': 'sof',\n",
       "   'prefix-4': 'soft',\n",
       "   'prefix-5': 'soft',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ft',\n",
       "   'suffix-3': 'oft',\n",
       "   'suffix-4': 'soft',\n",
       "   'suffix-5': 'soft',\n",
       "   'prev_word': 'very',\n",
       "   'next_word': '',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'Goooood',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'soft',\n",
       "   'word.ispunctuation': False}],\n",
       " [{'word': '!',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '!',\n",
       "   'prefix-2': '!',\n",
       "   'prefix-3': '!',\n",
       "   'prefix-4': '!',\n",
       "   'prefix-5': '!',\n",
       "   'suffix-1': '!',\n",
       "   'suffix-2': '!',\n",
       "   'suffix-3': '!',\n",
       "   'suffix-4': '!',\n",
       "   'suffix-5': '!',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'nice',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'to',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'wear',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '!',\n",
       "   'word.ispunctuation': True,\n",
       "   'BOS': True},\n",
       "  {'word': 'nice',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'n',\n",
       "   'prefix-2': 'ni',\n",
       "   'prefix-3': 'nic',\n",
       "   'prefix-4': 'nice',\n",
       "   'prefix-5': 'nice',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ce',\n",
       "   'suffix-3': 'ice',\n",
       "   'suffix-4': 'nice',\n",
       "   'suffix-5': 'nice',\n",
       "   'prev_word': '!',\n",
       "   'next_word': 'to',\n",
       "   'prev2_word': 'time',\n",
       "   'next2_word': 'wear',\n",
       "   'prev3_word': 'next',\n",
       "   'next3_word': 'Can',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'nic',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'to',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'to',\n",
       "   'prefix-3': 'to',\n",
       "   'prefix-4': 'to',\n",
       "   'prefix-5': 'to',\n",
       "   'suffix-1': 'o',\n",
       "   'suffix-2': 'to',\n",
       "   'suffix-3': 'to',\n",
       "   'suffix-4': 'to',\n",
       "   'suffix-5': 'to',\n",
       "   'prev_word': 'nice',\n",
       "   'next_word': 'wear',\n",
       "   'prev2_word': '!',\n",
       "   'next2_word': 'Can',\n",
       "   'prev3_word': 'time',\n",
       "   'next3_word': 'repeat',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'to',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'wear',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'we',\n",
       "   'prefix-3': 'wea',\n",
       "   'prefix-4': 'wear',\n",
       "   'prefix-5': 'wear',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'ar',\n",
       "   'suffix-3': 'ear',\n",
       "   'suffix-4': 'wear',\n",
       "   'suffix-5': 'wear',\n",
       "   'prev_word': 'to',\n",
       "   'next_word': 'Can',\n",
       "   'prev2_word': 'nice',\n",
       "   'next2_word': 'repeat',\n",
       "   'prev3_word': '!',\n",
       "   'next3_word': 'order',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'wear',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'Can',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'C',\n",
       "   'prefix-2': 'Ca',\n",
       "   'prefix-3': 'Can',\n",
       "   'prefix-4': 'Can',\n",
       "   'prefix-5': 'Can',\n",
       "   'suffix-1': 'n',\n",
       "   'suffix-2': 'an',\n",
       "   'suffix-3': 'Can',\n",
       "   'suffix-4': 'Can',\n",
       "   'suffix-5': 'Can',\n",
       "   'prev_word': 'wear',\n",
       "   'next_word': 'repeat',\n",
       "   'prev2_word': 'to',\n",
       "   'next2_word': 'order',\n",
       "   'prev3_word': 'nice',\n",
       "   'next3_word': 'next',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Can'],\n",
       "   'initcapsalpha': ['Can'],\n",
       "   'word.stemmed': 'ca',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'repeat',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'r',\n",
       "   'prefix-2': 're',\n",
       "   'prefix-3': 'rep',\n",
       "   'prefix-4': 'repe',\n",
       "   'prefix-5': 'repea',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'at',\n",
       "   'suffix-3': 'eat',\n",
       "   'suffix-4': 'peat',\n",
       "   'suffix-5': 'epeat',\n",
       "   'prev_word': 'Can',\n",
       "   'next_word': 'order',\n",
       "   'prev2_word': 'wear',\n",
       "   'next2_word': 'next',\n",
       "   'prev3_word': 'to',\n",
       "   'next3_word': 'time',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'repeat',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'order',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'o',\n",
       "   'prefix-2': 'or',\n",
       "   'prefix-3': 'ord',\n",
       "   'prefix-4': 'orde',\n",
       "   'prefix-5': 'order',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'er',\n",
       "   'suffix-3': 'der',\n",
       "   'suffix-4': 'rder',\n",
       "   'suffix-5': 'order',\n",
       "   'prev_word': 'repeat',\n",
       "   'next_word': 'next',\n",
       "   'prev2_word': 'Can',\n",
       "   'next2_word': 'time',\n",
       "   'prev3_word': 'wear',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'order',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'next',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'n',\n",
       "   'prefix-2': 'ne',\n",
       "   'prefix-3': 'nex',\n",
       "   'prefix-4': 'next',\n",
       "   'prefix-5': 'next',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'xt',\n",
       "   'suffix-3': 'ext',\n",
       "   'suffix-4': 'next',\n",
       "   'suffix-5': 'next',\n",
       "   'prev_word': 'order',\n",
       "   'next_word': 'time',\n",
       "   'prev2_word': 'repeat',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'Can',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'next',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'time',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'ti',\n",
       "   'prefix-3': 'tim',\n",
       "   'prefix-4': 'time',\n",
       "   'prefix-5': 'time',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'me',\n",
       "   'suffix-3': 'ime',\n",
       "   'suffix-4': 'time',\n",
       "   'suffix-5': 'time',\n",
       "   'prev_word': 'next',\n",
       "   'next_word': '',\n",
       "   'prev2_word': 'order',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'repeat',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'tim',\n",
       "   'word.ispunctuation': False}],\n",
       " [{'word': 'item',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'i',\n",
       "   'prefix-2': 'it',\n",
       "   'prefix-3': 'ite',\n",
       "   'prefix-4': 'item',\n",
       "   'prefix-5': 'item',\n",
       "   'suffix-1': 'm',\n",
       "   'suffix-2': 'em',\n",
       "   'suffix-3': 'tem',\n",
       "   'suffix-4': 'item',\n",
       "   'suffix-5': 'item',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'with',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'nice',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'packing',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'item',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': 'with',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'wi',\n",
       "   'prefix-3': 'wit',\n",
       "   'prefix-4': 'with',\n",
       "   'prefix-5': 'with',\n",
       "   'suffix-1': 'h',\n",
       "   'suffix-2': 'th',\n",
       "   'suffix-3': 'ith',\n",
       "   'suffix-4': 'with',\n",
       "   'suffix-5': 'with',\n",
       "   'prev_word': 'item',\n",
       "   'next_word': 'nice',\n",
       "   'prev2_word': '.',\n",
       "   'next2_word': 'packing',\n",
       "   'prev3_word': 'seller',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'with',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'nice',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'n',\n",
       "   'prefix-2': 'ni',\n",
       "   'prefix-3': 'nic',\n",
       "   'prefix-4': 'nice',\n",
       "   'prefix-5': 'nice',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ce',\n",
       "   'suffix-3': 'ice',\n",
       "   'suffix-4': 'nice',\n",
       "   'suffix-5': 'nice',\n",
       "   'prev_word': 'with',\n",
       "   'next_word': 'packing',\n",
       "   'prev2_word': 'item',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': '.',\n",
       "   'next3_word': 'good',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'nic',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'packing',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'p',\n",
       "   'prefix-2': 'pa',\n",
       "   'prefix-3': 'pac',\n",
       "   'prefix-4': 'pack',\n",
       "   'prefix-5': 'packi',\n",
       "   'suffix-1': 'g',\n",
       "   'suffix-2': 'ng',\n",
       "   'suffix-3': 'ing',\n",
       "   'suffix-4': 'king',\n",
       "   'suffix-5': 'cking',\n",
       "   'prev_word': 'nice',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'with',\n",
       "   'next2_word': 'good',\n",
       "   'prev3_word': 'item',\n",
       "   'next3_word': 'quality',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'pack',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'packing',\n",
       "   'next_word': 'good',\n",
       "   'prev2_word': 'nice',\n",
       "   'next2_word': 'quality',\n",
       "   'prev3_word': 'with',\n",
       "   'next3_word': 'product',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'good',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'g',\n",
       "   'prefix-2': 'go',\n",
       "   'prefix-3': 'goo',\n",
       "   'prefix-4': 'good',\n",
       "   'prefix-5': 'good',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'od',\n",
       "   'suffix-3': 'ood',\n",
       "   'suffix-4': 'good',\n",
       "   'suffix-5': 'good',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'quality',\n",
       "   'prev2_word': 'packing',\n",
       "   'next2_word': 'product',\n",
       "   'prev3_word': 'nice',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'good',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'quality',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'q',\n",
       "   'prefix-2': 'qu',\n",
       "   'prefix-3': 'qua',\n",
       "   'prefix-4': 'qual',\n",
       "   'prefix-5': 'quali',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'ty',\n",
       "   'suffix-3': 'ity',\n",
       "   'suffix-4': 'lity',\n",
       "   'suffix-5': 'ality',\n",
       "   'prev_word': 'good',\n",
       "   'next_word': 'product',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': 'packing',\n",
       "   'next3_word': 'good',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'qualit',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'product',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'p',\n",
       "   'prefix-2': 'pr',\n",
       "   'prefix-3': 'pro',\n",
       "   'prefix-4': 'prod',\n",
       "   'prefix-5': 'produ',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ct',\n",
       "   'suffix-3': 'uct',\n",
       "   'suffix-4': 'duct',\n",
       "   'suffix-5': 'oduct',\n",
       "   'prev_word': 'quality',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'good',\n",
       "   'next2_word': 'good',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'value',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'product',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'product',\n",
       "   'next_word': 'good',\n",
       "   'prev2_word': 'quality',\n",
       "   'next2_word': 'value',\n",
       "   'prev3_word': 'good',\n",
       "   'next3_word': 'for',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'good',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'g',\n",
       "   'prefix-2': 'go',\n",
       "   'prefix-3': 'goo',\n",
       "   'prefix-4': 'good',\n",
       "   'prefix-5': 'good',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'od',\n",
       "   'suffix-3': 'ood',\n",
       "   'suffix-4': 'good',\n",
       "   'suffix-5': 'good',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'value',\n",
       "   'prev2_word': 'product',\n",
       "   'next2_word': 'for',\n",
       "   'prev3_word': 'quality',\n",
       "   'next3_word': 'money',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'good',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'value',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'v',\n",
       "   'prefix-2': 'va',\n",
       "   'prefix-3': 'val',\n",
       "   'prefix-4': 'valu',\n",
       "   'prefix-5': 'value',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ue',\n",
       "   'suffix-3': 'lue',\n",
       "   'suffix-4': 'alue',\n",
       "   'suffix-5': 'value',\n",
       "   'prev_word': 'good',\n",
       "   'next_word': 'for',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': 'money',\n",
       "   'prev3_word': 'product',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'val',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'for',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'f',\n",
       "   'prefix-2': 'fo',\n",
       "   'prefix-3': 'for',\n",
       "   'prefix-4': 'for',\n",
       "   'prefix-5': 'for',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'or',\n",
       "   'suffix-3': 'for',\n",
       "   'suffix-4': 'for',\n",
       "   'suffix-5': 'for',\n",
       "   'prev_word': 'value',\n",
       "   'next_word': 'money',\n",
       "   'prev2_word': 'good',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'Excellent',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'for',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'money',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'm',\n",
       "   'prefix-2': 'mo',\n",
       "   'prefix-3': 'mon',\n",
       "   'prefix-4': 'mone',\n",
       "   'prefix-5': 'money',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'ey',\n",
       "   'suffix-3': 'ney',\n",
       "   'suffix-4': 'oney',\n",
       "   'suffix-5': 'money',\n",
       "   'prev_word': 'for',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'value',\n",
       "   'next2_word': 'Excellent',\n",
       "   'prev3_word': 'good',\n",
       "   'next3_word': 'service',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'mo',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'money',\n",
       "   'next_word': 'Excellent',\n",
       "   'prev2_word': 'for',\n",
       "   'next2_word': 'service',\n",
       "   'prev3_word': 'value',\n",
       "   'next3_word': 'by',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'Excellent',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'E',\n",
       "   'prefix-2': 'Ex',\n",
       "   'prefix-3': 'Exc',\n",
       "   'prefix-4': 'Exce',\n",
       "   'prefix-5': 'Excel',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'nt',\n",
       "   'suffix-3': 'ent',\n",
       "   'suffix-4': 'lent',\n",
       "   'suffix-5': 'llent',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'service',\n",
       "   'prev2_word': 'money',\n",
       "   'next2_word': 'by',\n",
       "   'prev3_word': 'for',\n",
       "   'next3_word': 'seller',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Excellent'],\n",
       "   'initcapsalpha': ['Excellent'],\n",
       "   'word.stemmed': 'excellent',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'service',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'se',\n",
       "   'prefix-3': 'ser',\n",
       "   'prefix-4': 'serv',\n",
       "   'prefix-5': 'servi',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ce',\n",
       "   'suffix-3': 'ice',\n",
       "   'suffix-4': 'vice',\n",
       "   'suffix-5': 'rvice',\n",
       "   'prev_word': 'Excellent',\n",
       "   'next_word': 'by',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': 'seller',\n",
       "   'prev3_word': 'money',\n",
       "   'next3_word': '.',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'servic',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'by',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'b',\n",
       "   'prefix-2': 'by',\n",
       "   'prefix-3': 'by',\n",
       "   'prefix-4': 'by',\n",
       "   'prefix-5': 'by',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'by',\n",
       "   'suffix-3': 'by',\n",
       "   'suffix-4': 'by',\n",
       "   'suffix-5': 'by',\n",
       "   'prev_word': 'service',\n",
       "   'next_word': 'seller',\n",
       "   'prev2_word': 'Excellent',\n",
       "   'next2_word': '.',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'by',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'seller',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'se',\n",
       "   'prefix-3': 'sel',\n",
       "   'prefix-4': 'sell',\n",
       "   'prefix-5': 'selle',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'er',\n",
       "   'suffix-3': 'ler',\n",
       "   'suffix-4': 'ller',\n",
       "   'suffix-5': 'eller',\n",
       "   'prev_word': 'by',\n",
       "   'next_word': '.',\n",
       "   'prev2_word': 'service',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'Excellent',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'seller',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': '.',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '.',\n",
       "   'prefix-2': '.',\n",
       "   'prefix-3': '.',\n",
       "   'prefix-4': '.',\n",
       "   'prefix-5': '.',\n",
       "   'suffix-1': '.',\n",
       "   'suffix-2': '.',\n",
       "   'suffix-3': '.',\n",
       "   'suffix-4': '.',\n",
       "   'suffix-5': '.',\n",
       "   'prev_word': 'seller',\n",
       "   'next_word': '',\n",
       "   'prev2_word': 'by',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'service',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '.',\n",
       "   'word.ispunctuation': True}],\n",
       " [{'word': 'I',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'I',\n",
       "   'prefix-2': 'I',\n",
       "   'prefix-3': 'I',\n",
       "   'prefix-4': 'I',\n",
       "   'prefix-5': 'I',\n",
       "   'suffix-1': 'I',\n",
       "   'suffix-2': 'I',\n",
       "   'suffix-3': 'I',\n",
       "   'suffix-4': 'I',\n",
       "   'suffix-5': 'I',\n",
       "   'prev_word': '',\n",
       "   'next_word': \"can\\\\'t\",\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'search',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'anymore',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'i',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': \"can\\\\'t\",\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'c',\n",
       "   'prefix-2': 'ca',\n",
       "   'prefix-3': 'can',\n",
       "   'prefix-4': 'can\\\\',\n",
       "   'prefix-5': \"can\\\\'\",\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': \"'t\",\n",
       "   'suffix-3': \"\\\\'t\",\n",
       "   'suffix-4': \"n\\\\'t\",\n",
       "   'suffix-5': \"an\\\\'t\",\n",
       "   'prev_word': 'I',\n",
       "   'next_word': 'search',\n",
       "   'prev2_word': '?',\n",
       "   'next2_word': 'anymore',\n",
       "   'prev3_word': '?',\n",
       "   'next3_word': 'Jollibee',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': \"can\\\\'t\",\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'search',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'se',\n",
       "   'prefix-3': 'sea',\n",
       "   'prefix-4': 'sear',\n",
       "   'prefix-5': 'searc',\n",
       "   'suffix-1': 'h',\n",
       "   'suffix-2': 'ch',\n",
       "   'suffix-3': 'rch',\n",
       "   'suffix-4': 'arch',\n",
       "   'suffix-5': 'earch',\n",
       "   'prev_word': \"can\\\\'t\",\n",
       "   'next_word': 'anymore',\n",
       "   'prev2_word': 'I',\n",
       "   'next2_word': 'Jollibee',\n",
       "   'prev3_word': '?',\n",
       "   'next3_word': 'store',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'search',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'anymore',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'a',\n",
       "   'prefix-2': 'an',\n",
       "   'prefix-3': 'any',\n",
       "   'prefix-4': 'anym',\n",
       "   'prefix-5': 'anymo',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 're',\n",
       "   'suffix-3': 'ore',\n",
       "   'suffix-4': 'more',\n",
       "   'suffix-5': 'ymore',\n",
       "   'prev_word': 'search',\n",
       "   'next_word': 'Jollibee',\n",
       "   'prev2_word': \"can\\\\'t\",\n",
       "   'next2_word': 'store',\n",
       "   'prev3_word': 'I',\n",
       "   'next3_word': 'near',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'anymor',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'Jollibee',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'J',\n",
       "   'prefix-2': 'Jo',\n",
       "   'prefix-3': 'Jol',\n",
       "   'prefix-4': 'Joll',\n",
       "   'prefix-5': 'Jolli',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ee',\n",
       "   'suffix-3': 'bee',\n",
       "   'suffix-4': 'ibee',\n",
       "   'suffix-5': 'libee',\n",
       "   'prev_word': 'anymore',\n",
       "   'next_word': 'store',\n",
       "   'prev2_word': 'search',\n",
       "   'next2_word': 'near',\n",
       "   'prev3_word': \"can\\\\'t\",\n",
       "   'next3_word': 'me',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Jollibee'],\n",
       "   'initcapsalpha': ['Jollibee'],\n",
       "   'word.stemmed': 'jollib',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'store',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'st',\n",
       "   'prefix-3': 'sto',\n",
       "   'prefix-4': 'stor',\n",
       "   'prefix-5': 'store',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 're',\n",
       "   'suffix-3': 'ore',\n",
       "   'suffix-4': 'tore',\n",
       "   'suffix-5': 'store',\n",
       "   'prev_word': 'Jollibee',\n",
       "   'next_word': 'near',\n",
       "   'prev2_word': 'anymore',\n",
       "   'next2_word': 'me',\n",
       "   'prev3_word': 'search',\n",
       "   'next3_word': 'I',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'stor',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'near',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'n',\n",
       "   'prefix-2': 'ne',\n",
       "   'prefix-3': 'nea',\n",
       "   'prefix-4': 'near',\n",
       "   'prefix-5': 'near',\n",
       "   'suffix-1': 'r',\n",
       "   'suffix-2': 'ar',\n",
       "   'suffix-3': 'ear',\n",
       "   'suffix-4': 'near',\n",
       "   'suffix-5': 'near',\n",
       "   'prev_word': 'store',\n",
       "   'next_word': 'me',\n",
       "   'prev2_word': 'Jollibee',\n",
       "   'next2_word': 'I',\n",
       "   'prev3_word': 'anymore',\n",
       "   'next3_word': 'had',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'near',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'me',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'm',\n",
       "   'prefix-2': 'me',\n",
       "   'prefix-3': 'me',\n",
       "   'prefix-4': 'me',\n",
       "   'prefix-5': 'me',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'me',\n",
       "   'suffix-3': 'me',\n",
       "   'suffix-4': 'me',\n",
       "   'suffix-5': 'me',\n",
       "   'prev_word': 'near',\n",
       "   'next_word': 'I',\n",
       "   'prev2_word': 'store',\n",
       "   'next2_word': 'had',\n",
       "   'prev3_word': 'Jollibee',\n",
       "   'next3_word': 'ordered',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'me',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'I',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'I',\n",
       "   'prefix-2': 'I',\n",
       "   'prefix-3': 'I',\n",
       "   'prefix-4': 'I',\n",
       "   'prefix-5': 'I',\n",
       "   'suffix-1': 'I',\n",
       "   'suffix-2': 'I',\n",
       "   'suffix-3': 'I',\n",
       "   'suffix-4': 'I',\n",
       "   'suffix-5': 'I',\n",
       "   'prev_word': 'me',\n",
       "   'next_word': 'had',\n",
       "   'prev2_word': 'near',\n",
       "   'next2_word': 'ordered',\n",
       "   'prev3_word': 'store',\n",
       "   'next3_word': 'last',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'i',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'had',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'h',\n",
       "   'prefix-2': 'ha',\n",
       "   'prefix-3': 'had',\n",
       "   'prefix-4': 'had',\n",
       "   'prefix-5': 'had',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'ad',\n",
       "   'suffix-3': 'had',\n",
       "   'suffix-4': 'had',\n",
       "   'suffix-5': 'had',\n",
       "   'prev_word': 'I',\n",
       "   'next_word': 'ordered',\n",
       "   'prev2_word': 'me',\n",
       "   'next2_word': 'last',\n",
       "   'prev3_word': 'near',\n",
       "   'next3_word': 'week',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'had',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'ordered',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'o',\n",
       "   'prefix-2': 'or',\n",
       "   'prefix-3': 'ord',\n",
       "   'prefix-4': 'orde',\n",
       "   'prefix-5': 'order',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'ed',\n",
       "   'suffix-3': 'red',\n",
       "   'suffix-4': 'ered',\n",
       "   'suffix-5': 'dered',\n",
       "   'prev_word': 'had',\n",
       "   'next_word': 'last',\n",
       "   'prev2_word': 'I',\n",
       "   'next2_word': 'week',\n",
       "   'prev3_word': 'me',\n",
       "   'next3_word': 'but',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'ordered',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'last',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'l',\n",
       "   'prefix-2': 'la',\n",
       "   'prefix-3': 'las',\n",
       "   'prefix-4': 'last',\n",
       "   'prefix-5': 'last',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'st',\n",
       "   'suffix-3': 'ast',\n",
       "   'suffix-4': 'last',\n",
       "   'suffix-5': 'last',\n",
       "   'prev_word': 'ordered',\n",
       "   'next_word': 'week',\n",
       "   'prev2_word': 'had',\n",
       "   'next2_word': 'but',\n",
       "   'prev3_word': 'I',\n",
       "   'next3_word': 'now',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'last',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'week',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'we',\n",
       "   'prefix-3': 'wee',\n",
       "   'prefix-4': 'week',\n",
       "   'prefix-5': 'week',\n",
       "   'suffix-1': 'k',\n",
       "   'suffix-2': 'ek',\n",
       "   'suffix-3': 'eek',\n",
       "   'suffix-4': 'week',\n",
       "   'suffix-5': 'week',\n",
       "   'prev_word': 'last',\n",
       "   'next_word': 'but',\n",
       "   'prev2_word': 'ordered',\n",
       "   'next2_word': 'now',\n",
       "   'prev3_word': 'had',\n",
       "   'next3_word': '?',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'week',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'but',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'b',\n",
       "   'prefix-2': 'bu',\n",
       "   'prefix-3': 'but',\n",
       "   'prefix-4': 'but',\n",
       "   'prefix-5': 'but',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ut',\n",
       "   'suffix-3': 'but',\n",
       "   'suffix-4': 'but',\n",
       "   'suffix-5': 'but',\n",
       "   'prev_word': 'week',\n",
       "   'next_word': 'now',\n",
       "   'prev2_word': 'last',\n",
       "   'next2_word': '?',\n",
       "   'prev3_word': 'ordered',\n",
       "   'next3_word': '?',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'but',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'now',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'n',\n",
       "   'prefix-2': 'no',\n",
       "   'prefix-3': 'now',\n",
       "   'prefix-4': 'now',\n",
       "   'prefix-5': 'now',\n",
       "   'suffix-1': 'w',\n",
       "   'suffix-2': 'ow',\n",
       "   'suffix-3': 'now',\n",
       "   'suffix-4': 'now',\n",
       "   'suffix-5': 'now',\n",
       "   'prev_word': 'but',\n",
       "   'next_word': '?',\n",
       "   'prev2_word': 'week',\n",
       "   'next2_word': '?',\n",
       "   'prev3_word': 'last',\n",
       "   'next3_word': '?',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'now',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': '?',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '?',\n",
       "   'prefix-2': '?',\n",
       "   'prefix-3': '?',\n",
       "   'prefix-4': '?',\n",
       "   'prefix-5': '?',\n",
       "   'suffix-1': '?',\n",
       "   'suffix-2': '?',\n",
       "   'suffix-3': '?',\n",
       "   'suffix-4': '?',\n",
       "   'suffix-5': '?',\n",
       "   'prev_word': 'now',\n",
       "   'next_word': '?',\n",
       "   'prev2_word': 'but',\n",
       "   'next2_word': '?',\n",
       "   'prev3_word': 'week',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '?',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': '?',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '?',\n",
       "   'prefix-2': '?',\n",
       "   'prefix-3': '?',\n",
       "   'prefix-4': '?',\n",
       "   'prefix-5': '?',\n",
       "   'suffix-1': '?',\n",
       "   'suffix-2': '?',\n",
       "   'suffix-3': '?',\n",
       "   'suffix-4': '?',\n",
       "   'suffix-5': '?',\n",
       "   'prev_word': '?',\n",
       "   'next_word': '?',\n",
       "   'prev2_word': 'now',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'but',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '?',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': '?',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '?',\n",
       "   'prefix-2': '?',\n",
       "   'prefix-3': '?',\n",
       "   'prefix-4': '?',\n",
       "   'prefix-5': '?',\n",
       "   'suffix-1': '?',\n",
       "   'suffix-2': '?',\n",
       "   'suffix-3': '?',\n",
       "   'suffix-4': '?',\n",
       "   'suffix-5': '?',\n",
       "   'prev_word': '?',\n",
       "   'next_word': '',\n",
       "   'prev2_word': '?',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'now',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '?',\n",
       "   'word.ispunctuation': True}],\n",
       " [{'word': 'Fast',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'F',\n",
       "   'prefix-2': 'Fa',\n",
       "   'prefix-3': 'Fas',\n",
       "   'prefix-4': 'Fast',\n",
       "   'prefix-5': 'Fast',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'st',\n",
       "   'suffix-3': 'ast',\n",
       "   'suffix-4': 'Fast',\n",
       "   'suffix-5': 'Fast',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'delivery',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'good',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Fast'],\n",
       "   'initcapsalpha': ['Fast'],\n",
       "   'word.stemmed': 'fast',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': 'delivery',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'd',\n",
       "   'prefix-2': 'de',\n",
       "   'prefix-3': 'del',\n",
       "   'prefix-4': 'deli',\n",
       "   'prefix-5': 'deliv',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'ry',\n",
       "   'suffix-3': 'ery',\n",
       "   'suffix-4': 'very',\n",
       "   'suffix-5': 'ivery',\n",
       "   'prev_word': 'Fast',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': 'service',\n",
       "   'next2_word': 'good',\n",
       "   'prev3_word': 'Excellent',\n",
       "   'next3_word': 'packing',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'deliver',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'delivery',\n",
       "   'next_word': 'good',\n",
       "   'prev2_word': 'Fast',\n",
       "   'next2_word': 'packing',\n",
       "   'prev3_word': 'service',\n",
       "   'next3_word': ',',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'good',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'g',\n",
       "   'prefix-2': 'go',\n",
       "   'prefix-3': 'goo',\n",
       "   'prefix-4': 'good',\n",
       "   'prefix-5': 'good',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'od',\n",
       "   'suffix-3': 'ood',\n",
       "   'suffix-4': 'good',\n",
       "   'suffix-5': 'good',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'packing',\n",
       "   'prev2_word': 'delivery',\n",
       "   'next2_word': ',',\n",
       "   'prev3_word': 'Fast',\n",
       "   'next3_word': 'good',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'good',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'packing',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'p',\n",
       "   'prefix-2': 'pa',\n",
       "   'prefix-3': 'pac',\n",
       "   'prefix-4': 'pack',\n",
       "   'prefix-5': 'packi',\n",
       "   'suffix-1': 'g',\n",
       "   'suffix-2': 'ng',\n",
       "   'suffix-3': 'ing',\n",
       "   'suffix-4': 'king',\n",
       "   'suffix-5': 'cking',\n",
       "   'prev_word': 'good',\n",
       "   'next_word': ',',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': 'good',\n",
       "   'prev3_word': 'delivery',\n",
       "   'next3_word': 'value',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'pack',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': ',',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': ',',\n",
       "   'prefix-2': ',',\n",
       "   'prefix-3': ',',\n",
       "   'prefix-4': ',',\n",
       "   'prefix-5': ',',\n",
       "   'suffix-1': ',',\n",
       "   'suffix-2': ',',\n",
       "   'suffix-3': ',',\n",
       "   'suffix-4': ',',\n",
       "   'suffix-5': ',',\n",
       "   'prev_word': 'packing',\n",
       "   'next_word': 'good',\n",
       "   'prev2_word': 'good',\n",
       "   'next2_word': 'value',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'product',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': ',',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'good',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'g',\n",
       "   'prefix-2': 'go',\n",
       "   'prefix-3': 'goo',\n",
       "   'prefix-4': 'good',\n",
       "   'prefix-5': 'good',\n",
       "   'suffix-1': 'd',\n",
       "   'suffix-2': 'od',\n",
       "   'suffix-3': 'ood',\n",
       "   'suffix-4': 'good',\n",
       "   'suffix-5': 'good',\n",
       "   'prev_word': ',',\n",
       "   'next_word': 'value',\n",
       "   'prev2_word': 'packing',\n",
       "   'next2_word': 'product',\n",
       "   'prev3_word': 'good',\n",
       "   'next3_word': '.',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'good',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'value',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'v',\n",
       "   'prefix-2': 'va',\n",
       "   'prefix-3': 'val',\n",
       "   'prefix-4': 'valu',\n",
       "   'prefix-5': 'value',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ue',\n",
       "   'suffix-3': 'lue',\n",
       "   'suffix-4': 'alue',\n",
       "   'suffix-5': 'value',\n",
       "   'prev_word': 'good',\n",
       "   'next_word': 'product',\n",
       "   'prev2_word': ',',\n",
       "   'next2_word': '.',\n",
       "   'prev3_word': 'packing',\n",
       "   'next3_word': 'Excellent',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'val',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'product',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'p',\n",
       "   'prefix-2': 'pr',\n",
       "   'prefix-3': 'pro',\n",
       "   'prefix-4': 'prod',\n",
       "   'prefix-5': 'produ',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ct',\n",
       "   'suffix-3': 'uct',\n",
       "   'suffix-4': 'duct',\n",
       "   'suffix-5': 'oduct',\n",
       "   'prev_word': 'value',\n",
       "   'next_word': '.',\n",
       "   'prev2_word': 'good',\n",
       "   'next2_word': 'Excellent',\n",
       "   'prev3_word': ',',\n",
       "   'next3_word': 'service',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'product',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': '.',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '.',\n",
       "   'prefix-2': '.',\n",
       "   'prefix-3': '.',\n",
       "   'prefix-4': '.',\n",
       "   'prefix-5': '.',\n",
       "   'suffix-1': '.',\n",
       "   'suffix-2': '.',\n",
       "   'suffix-3': '.',\n",
       "   'suffix-4': '.',\n",
       "   'suffix-5': '.',\n",
       "   'prev_word': 'product',\n",
       "   'next_word': 'Excellent',\n",
       "   'prev2_word': 'value',\n",
       "   'next2_word': 'service',\n",
       "   'prev3_word': 'good',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '.',\n",
       "   'word.ispunctuation': True},\n",
       "  {'word': 'Excellent',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'E',\n",
       "   'prefix-2': 'Ex',\n",
       "   'prefix-3': 'Exc',\n",
       "   'prefix-4': 'Exce',\n",
       "   'prefix-5': 'Excel',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'nt',\n",
       "   'suffix-3': 'ent',\n",
       "   'suffix-4': 'lent',\n",
       "   'suffix-5': 'llent',\n",
       "   'prev_word': '.',\n",
       "   'next_word': 'service',\n",
       "   'prev2_word': 'product',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'value',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Excellent'],\n",
       "   'initcapsalpha': ['Excellent'],\n",
       "   'word.stemmed': 'excellent',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'service',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'se',\n",
       "   'prefix-3': 'ser',\n",
       "   'prefix-4': 'serv',\n",
       "   'prefix-5': 'servi',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ce',\n",
       "   'suffix-3': 'ice',\n",
       "   'suffix-4': 'vice',\n",
       "   'suffix-5': 'rvice',\n",
       "   'prev_word': 'Excellent',\n",
       "   'next_word': '',\n",
       "   'prev2_word': '.',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'product',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'servic',\n",
       "   'word.ispunctuation': False}],\n",
       " [{'word': 'Not',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'N',\n",
       "   'prefix-2': 'No',\n",
       "   'prefix-3': 'Not',\n",
       "   'prefix-4': 'Not',\n",
       "   'prefix-5': 'Not',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ot',\n",
       "   'suffix-3': 'Not',\n",
       "   'suffix-4': 'Not',\n",
       "   'suffix-5': 'Not',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'try',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'yet',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': '.',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Not'],\n",
       "   'initcapsalpha': ['Not'],\n",
       "   'word.stemmed': 'not',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': 'try',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 't',\n",
       "   'prefix-2': 'tr',\n",
       "   'prefix-3': 'try',\n",
       "   'prefix-4': 'try',\n",
       "   'prefix-5': 'try',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'ry',\n",
       "   'suffix-3': 'try',\n",
       "   'suffix-4': 'try',\n",
       "   'suffix-5': 'try',\n",
       "   'prev_word': 'Not',\n",
       "   'next_word': 'yet',\n",
       "   'prev2_word': '.',\n",
       "   'next2_word': '.',\n",
       "   'prev3_word': 'yet',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'tr',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'yet',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'y',\n",
       "   'prefix-2': 'ye',\n",
       "   'prefix-3': 'yet',\n",
       "   'prefix-4': 'yet',\n",
       "   'prefix-5': 'yet',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'et',\n",
       "   'suffix-3': 'yet',\n",
       "   'suffix-4': 'yet',\n",
       "   'suffix-5': 'yet',\n",
       "   'prev_word': 'try',\n",
       "   'next_word': '.',\n",
       "   'prev2_word': 'Not',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': '.',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'yet',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': '.',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '.',\n",
       "   'prefix-2': '.',\n",
       "   'prefix-3': '.',\n",
       "   'prefix-4': '.',\n",
       "   'prefix-5': '.',\n",
       "   'suffix-1': '.',\n",
       "   'suffix-2': '.',\n",
       "   'suffix-3': '.',\n",
       "   'suffix-4': '.',\n",
       "   'suffix-5': '.',\n",
       "   'prev_word': 'yet',\n",
       "   'next_word': '',\n",
       "   'prev2_word': 'try',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'Not',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '.',\n",
       "   'word.ispunctuation': True}],\n",
       " [{'word': 'But',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'B',\n",
       "   'prefix-2': 'Bu',\n",
       "   'prefix-3': 'But',\n",
       "   'prefix-4': 'But',\n",
       "   'prefix-5': 'But',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'ut',\n",
       "   'suffix-3': 'But',\n",
       "   'suffix-4': 'But',\n",
       "   'suffix-5': 'But',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'looks',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'like',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'just',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['But'],\n",
       "   'initcapsalpha': ['But'],\n",
       "   'word.stemmed': 'but',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': 'looks',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'l',\n",
       "   'prefix-2': 'lo',\n",
       "   'prefix-3': 'loo',\n",
       "   'prefix-4': 'look',\n",
       "   'prefix-5': 'looks',\n",
       "   'suffix-1': 's',\n",
       "   'suffix-2': 'ks',\n",
       "   'suffix-3': 'oks',\n",
       "   'suffix-4': 'ooks',\n",
       "   'suffix-5': 'looks',\n",
       "   'prev_word': 'But',\n",
       "   'next_word': 'like',\n",
       "   'prev2_word': '.',\n",
       "   'next2_word': 'just',\n",
       "   'prev3_word': 'nice',\n",
       "   'next3_word': 'nice',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'looks',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'like',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'l',\n",
       "   'prefix-2': 'li',\n",
       "   'prefix-3': 'lik',\n",
       "   'prefix-4': 'like',\n",
       "   'prefix-5': 'like',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ke',\n",
       "   'suffix-3': 'ike',\n",
       "   'suffix-4': 'like',\n",
       "   'suffix-5': 'like',\n",
       "   'prev_word': 'looks',\n",
       "   'next_word': 'just',\n",
       "   'prev2_word': 'But',\n",
       "   'next2_word': 'nice',\n",
       "   'prev3_word': '.',\n",
       "   'next3_word': '.',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'lik',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'just',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'j',\n",
       "   'prefix-2': 'ju',\n",
       "   'prefix-3': 'jus',\n",
       "   'prefix-4': 'just',\n",
       "   'prefix-5': 'just',\n",
       "   'suffix-1': 't',\n",
       "   'suffix-2': 'st',\n",
       "   'suffix-3': 'ust',\n",
       "   'suffix-4': 'just',\n",
       "   'suffix-5': 'just',\n",
       "   'prev_word': 'like',\n",
       "   'next_word': 'nice',\n",
       "   'prev2_word': 'looks',\n",
       "   'next2_word': '.',\n",
       "   'prev3_word': 'But',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'just',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'nice',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'n',\n",
       "   'prefix-2': 'ni',\n",
       "   'prefix-3': 'nic',\n",
       "   'prefix-4': 'nice',\n",
       "   'prefix-5': 'nice',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'ce',\n",
       "   'suffix-3': 'ice',\n",
       "   'suffix-4': 'nice',\n",
       "   'suffix-5': 'nice',\n",
       "   'prev_word': 'just',\n",
       "   'next_word': '.',\n",
       "   'prev2_word': 'like',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'looks',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'nic',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': '.',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '.',\n",
       "   'prefix-2': '.',\n",
       "   'prefix-3': '.',\n",
       "   'prefix-4': '.',\n",
       "   'prefix-5': '.',\n",
       "   'suffix-1': '.',\n",
       "   'suffix-2': '.',\n",
       "   'suffix-3': '.',\n",
       "   'suffix-4': '.',\n",
       "   'suffix-5': '.',\n",
       "   'prev_word': 'nice',\n",
       "   'next_word': '',\n",
       "   'prev2_word': 'just',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'like',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '.',\n",
       "   'word.ispunctuation': True}],\n",
       " [{'word': 'Suitable',\n",
       "   'is_first': True,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': False,\n",
       "   'prefix-1': 'S',\n",
       "   'prefix-2': 'Su',\n",
       "   'prefix-3': 'Sui',\n",
       "   'prefix-4': 'Suit',\n",
       "   'prefix-5': 'Suita',\n",
       "   'suffix-1': 'e',\n",
       "   'suffix-2': 'le',\n",
       "   'suffix-3': 'ble',\n",
       "   'suffix-4': 'able',\n",
       "   'suffix-5': 'table',\n",
       "   'prev_word': '',\n",
       "   'next_word': 'with',\n",
       "   'prev2_word': '',\n",
       "   'next2_word': 'my',\n",
       "   'prev3_word': '',\n",
       "   'next3_word': 'skin',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': ['Suitable'],\n",
       "   'initcapsalpha': ['Suitable'],\n",
       "   'word.stemmed': 'suitabl',\n",
       "   'word.ispunctuation': False,\n",
       "   'BOS': True},\n",
       "  {'word': 'with',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'w',\n",
       "   'prefix-2': 'wi',\n",
       "   'prefix-3': 'wit',\n",
       "   'prefix-4': 'with',\n",
       "   'prefix-5': 'with',\n",
       "   'suffix-1': 'h',\n",
       "   'suffix-2': 'th',\n",
       "   'suffix-3': 'ith',\n",
       "   'suffix-4': 'with',\n",
       "   'suffix-5': 'with',\n",
       "   'prev_word': 'Suitable',\n",
       "   'next_word': 'my',\n",
       "   'prev2_word': '.',\n",
       "   'next2_word': 'skin',\n",
       "   'prev3_word': 'skin',\n",
       "   'next3_word': '.',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'with',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'my',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 'm',\n",
       "   'prefix-2': 'my',\n",
       "   'prefix-3': 'my',\n",
       "   'prefix-4': 'my',\n",
       "   'prefix-5': 'my',\n",
       "   'suffix-1': 'y',\n",
       "   'suffix-2': 'my',\n",
       "   'suffix-3': 'my',\n",
       "   'suffix-4': 'my',\n",
       "   'suffix-5': 'my',\n",
       "   'prev_word': 'with',\n",
       "   'next_word': 'skin',\n",
       "   'prev2_word': 'Suitable',\n",
       "   'next2_word': '.',\n",
       "   'prev3_word': '.',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'my',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': 'skin',\n",
       "   'is_first': False,\n",
       "   'is_last': False,\n",
       "   'is_capitalized': False,\n",
       "   'is_all_caps': False,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': 's',\n",
       "   'prefix-2': 'sk',\n",
       "   'prefix-3': 'ski',\n",
       "   'prefix-4': 'skin',\n",
       "   'prefix-5': 'skin',\n",
       "   'suffix-1': 'n',\n",
       "   'suffix-2': 'in',\n",
       "   'suffix-3': 'kin',\n",
       "   'suffix-4': 'skin',\n",
       "   'suffix-5': 'skin',\n",
       "   'prev_word': 'my',\n",
       "   'next_word': '.',\n",
       "   'prev2_word': 'with',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'Suitable',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': 'sk',\n",
       "   'word.ispunctuation': False},\n",
       "  {'word': '.',\n",
       "   'is_first': False,\n",
       "   'is_last': True,\n",
       "   'is_capitalized': True,\n",
       "   'is_all_caps': True,\n",
       "   'is_all_lower': True,\n",
       "   'prefix-1': '.',\n",
       "   'prefix-2': '.',\n",
       "   'prefix-3': '.',\n",
       "   'prefix-4': '.',\n",
       "   'prefix-5': '.',\n",
       "   'suffix-1': '.',\n",
       "   'suffix-2': '.',\n",
       "   'suffix-3': '.',\n",
       "   'suffix-4': '.',\n",
       "   'suffix-5': '.',\n",
       "   'prev_word': 'skin',\n",
       "   'next_word': '',\n",
       "   'prev2_word': 'my',\n",
       "   'next2_word': '',\n",
       "   'prev3_word': 'with',\n",
       "   'next3_word': '',\n",
       "   'has_hyphen': False,\n",
       "   'is_numeric': False,\n",
       "   'capitals_inside': False,\n",
       "   'natural_number': [],\n",
       "   'initcaps': [],\n",
       "   'initcapsalpha': [],\n",
       "   'word.stemmed': '.',\n",
       "   'word.ispunctuation': True}]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e1a0c-e574-4af3-ae6b-f52f0ea3a07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1b7b13-9831-40cb-a804-5e6cb02a1c8e",
   "metadata": {},
   "source": [
    "# Trying to implement Karl Stratos Mini Tagger Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac60301-bca9-45a3-8a13-22e64e4532de",
   "metadata": {},
   "source": [
    "# Trying to split things out JIAYOU WINNIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "de1d550b-e36a-401f-a6da-6d3ef2445126",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRONT_BUFFER_SYMBOL = \"_START_\"  # For sentence boundaries\n",
    "END_BUFFER_SYMBOL = \"_END_\"  # For sentence boundaries\n",
    "UNKNOWN_SYMBOL = \"<?>\"   # For unknown observation types at test time\n",
    "\n",
    "global SPELLING_FEATURE_CACHE\n",
    "SPELLING_FEATURE_CACHE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c4eb5106-fb85-4ea9-9479-244ecde38550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_spelling_feature_cache():\n",
    "    \"\"\"Clears the global spelling feature cache.\"\"\"\n",
    "    global SPELLING_FEATURE_CACHE\n",
    "    SPELLING_FEATURE_CACHE = {}\n",
    "    \n",
    "def get_word(word_sequence, position):\n",
    "    \"\"\"Gets the word at the specified position.\"\"\"\n",
    "    if position < 0:\n",
    "        return FRONT_BUFFER_SYMBOL\n",
    "    elif position >= len(word_sequence):\n",
    "        return END_BUFFER_SYMBOL\n",
    "    else:\n",
    "        return word_sequence[position]\n",
    "\n",
    "def is_capitalized(word):\n",
    "    \"\"\"Is the word capitalized?\"\"\"\n",
    "    return word[0].isupper()\n",
    "\n",
    "def get_prefix(word, length):\n",
    "    \"\"\"Gets a padded prefix of the word up to the given length.\"\"\"\n",
    "    prefix = \"\"\n",
    "    for i in range(length):\n",
    "        if i < len(word):\n",
    "            prefix += word[i]\n",
    "        else:\n",
    "            prefix += \"*\"\n",
    "    return prefix\n",
    "\n",
    "def get_suffix(word, length):\n",
    "    \"\"\"Gets a padded suffix of the word up to the given length.\"\"\"\n",
    "    suffix = \"\"\n",
    "    for i in range(length):\n",
    "        if i < len(word):\n",
    "            suffix = word[-i-1] + suffix\n",
    "        else:\n",
    "            suffix = \"*\" + suffix\n",
    "    return suffix\n",
    "\n",
    "def is_all_nonalphanumeric(word):\n",
    "    \"\"\"Is the word all nonalphanumeric?\"\"\"\n",
    "    for char in word:\n",
    "        if char.isalnum():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_float(word):\n",
    "    \"\"\"Can the word be converted to a float (i.e., numeric value)?\"\"\"\n",
    "    try:\n",
    "        float(word)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def spelling_features(word, relative_position):\n",
    "    \"\"\"\n",
    "    Extracts spelling features about the given word. Also considers the word's\n",
    "    relative position.\n",
    "    \"\"\"\n",
    "    if not (word, relative_position) in SPELLING_FEATURE_CACHE:\n",
    "        features = {}\n",
    "        features[\"word({0})={1}\".format(relative_position, word)] = 1\n",
    "        features['is_capitalized({0})={1}'.format(\n",
    "                relative_position, is_capitalized(word))] = 1\n",
    "        for length in range(1, 5):\n",
    "            features[\"prefix{0}({1})={2}\".format(\n",
    "                    length, relative_position, get_prefix(word, length))] = 1\n",
    "            features[\"suffix{0}({1})={2}\".format(\n",
    "                    length, relative_position, get_suffix(word, length))] = 1\n",
    "        features[\"is_all_nonalphanumeric({0})={1}\".format(\n",
    "                relative_position, is_all_nonalphanumeric(word))] = 1\n",
    "        features[\"is_float({0})={1}\".format(\n",
    "                relative_position, is_float(word))] = 1\n",
    "        SPELLING_FEATURE_CACHE[(word, relative_position)] = features\n",
    "\n",
    "    # Return a copy so that modifying that object doesn't modify the cache.\n",
    "    return SPELLING_FEATURE_CACHE[(word, relative_position)].copy()\n",
    "\n",
    "def get_baseline_features(word_sequence, position): \n",
    "    word = get_word(word_sequence, position)\n",
    "    word_left1 = get_word(word_sequence, position - 1)\n",
    "    word_left2 = get_word(word_sequence, position - 2)\n",
    "    word_right1 = get_word(word_sequence, position + 1)\n",
    "    word_right2 = get_word(word_sequence, position + 2)\n",
    "    features = spelling_features(word, 0)\n",
    "    features[\"word(-1)={0}\".format(word_left1)] = 1\n",
    "    features[\"word(-2)={0}\".format(word_left2)] = 1\n",
    "    features[\"word(+1)={0}\".format(word_right1)] = 1\n",
    "    features[\"word(+2)={0}\".format(word_right2)] = 1\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4554ab8-3bcf-46f7-b1c7-568b6f30d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bitstring_features(word_sequence, position, bitstring_dictionary):\n",
    "    \"\"\"\n",
    "    Bit string features: baseline features + bit strings of current, left, and\n",
    "    right words.\n",
    "    \"\"\"\n",
    "    # Compute the baseline feature vector.\n",
    "    features = get_baseline_features(word_sequence, position)\n",
    "\n",
    "    # Add the bit string features.\n",
    "    word = word_sequence[position]  # current word\n",
    "    if word in bitstring_dictionary:\n",
    "        word_bitstring = bitstring_dictionary[word]\n",
    "    else:\n",
    "        word_bitstring = bitstring_dictionary[UNKNOWN_SYMBOL]\n",
    "    for i in range(1, len(word_bitstring) + 1):\n",
    "        features[\"bitstring(0)_prefix({0})={1}\".format(\n",
    "                i, word_bitstring[:i])] = 1\n",
    "    features[\"bitstring(0)_all={0}\".format(word_bitstring)] = 1\n",
    "\n",
    "    if position > 0:\n",
    "        word = word_sequence[position - 1]  # word to the left\n",
    "        if word in bitstring_dictionary:\n",
    "            word_bitstring = bitstring_dictionary[word]\n",
    "        else:\n",
    "            word_bitstring = bitstring_dictionary[UNKNOWN_SYMBOL]\n",
    "        for i in range(1, len(word_bitstring) + 1):\n",
    "            features[\"bitstring(-1)_prefix({0})={1}\".format(\n",
    "                    i, word_bitstring[:i])] = 1\n",
    "        features[\"bitstring(-1)_all={0}\".format(word_bitstring)] = 1\n",
    "\n",
    "    if position < len(word_sequence) - 1:\n",
    "        word = word_sequence[position + 1]  # word to the right\n",
    "        if word in bitstring_dictionary:\n",
    "            word_bitstring = bitstring_dictionary[word]\n",
    "        else:\n",
    "            word_bitstring = bitstring_dictionary[UNKNOWN_SYMBOL]\n",
    "        for i in range(1, len(word_bitstring) + 1):\n",
    "            features[\"bitstring(+1)_prefix({0})={1}\".format(\n",
    "                    i, word_bitstring[:i])] = 1\n",
    "        features[\"bitstring(+1)_all={0}\".format(word_bitstring)] = 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "79940973-9f0c-4385-b2ae-2711dfece831",
   "metadata": {},
   "outputs": [],
   "source": [
    "__map_label_str2num = {}\n",
    "__map_label_num2str = {}\n",
    "__map_feature_str2num = {}\n",
    "__map_feature_num2str = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "86cfb5e8-c90a-4290-8b21-d2ab151223a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"__map_label_str2num\", __map_label_str2num)\n",
    "print(\"__map_label_num2str\", __map_label_num2str)\n",
    "print(\"__map_feature_str2num\", __map_feature_str2num)\n",
    "print(\"__map_feature_num2str\", __map_feature_num2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d745d5d3-a942-44b3-a1cd-ca342f6765b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_label(label):\n",
    "    \"\"\"\n",
    "    When this function is being called, it will append a new label into the dictionary.\n",
    "    If the label already exist in the function, it will just return the label in integer value. \n",
    "    Universal tagset will return 1-12. \n",
    "    \"\"\"\n",
    "    if not label in __map_label_str2num:\n",
    "        label_number = len(__map_label_str2num) + 1  # index from 1\n",
    "        __map_label_str2num[label] = label_number\n",
    "        __map_label_num2str[label_number] = label\n",
    "    return __map_label_str2num[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "d133b460-93fa-44b7-8bc0-dda13acb752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT HERE TO CHANGE THE TEMPLATE TO EXTRACT FEATURES\n",
    "feature_template = \"baseline\" \n",
    "# feature_template = \"embedding\" \n",
    "# feature_template = \"bitstring\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "060d6627-c9f3-4adb-bd61-2b13811630df",
   "metadata": {},
   "outputs": [],
   "source": [
    "__word_embedding = None\n",
    "__word_bitstring = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e7225c33-6a8a-4001-b2a9-be2ccd27b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_feature_types():\n",
    "    return len(__map_feature_str2num)\n",
    "\n",
    "def get_feature_string(feature_number):\n",
    "    return __map_feature_num2str[feature_number]\n",
    "\n",
    "def get_label_string(label_number):\n",
    "    return __map_label_num2str[label_number]\n",
    "\n",
    "def get_feature_number(feature_string):\n",
    "    return __map_feature_str2num[feature_string]\n",
    "\n",
    "def get_label_number(label_string):\n",
    "    return __map_label_str2num[label_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "e5f6c134-7030-457b-8c22-d92acc4d15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_features(observation_sequence, position):\n",
    "    if feature_template == \"baseline\":\n",
    "        raw_features = get_baseline_features(observation_sequence, position)\n",
    "    elif feature_template == \"embedding\":\n",
    "        assert __word_embedding is not None\n",
    "        raw_features = get_embedding_features(observation_sequence, position, __word_embedding)\n",
    "    elif feature_template == \"bitstring\":\n",
    "        assert __word_bitstring is not None\n",
    "        raw_features = get_bitstring_features(observation_sequence, position, __word_bitstring)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported feature template {0}\".format(feature_template))\n",
    "        \n",
    "    numeric_features = {}\n",
    "    for raw_feature in raw_features:\n",
    "        \n",
    "        if not raw_feature in __map_feature_str2num:\n",
    "            feature_number = len(__map_feature_str2num) + 1\n",
    "            __map_feature_str2num[raw_feature] = feature_number\n",
    "            __map_feature_num2str[feature_number] = raw_feature\n",
    "            \n",
    "        numeric_features[__map_feature_str2num[raw_feature]] = raw_features[raw_feature]\n",
    "        \n",
    "    return numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "540651e6-58db-4a43-baff-b7f17440b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sequence_data, extract_all, skip_list): \n",
    "    label_list = []\n",
    "    features_list = []\n",
    "    location_list = []\n",
    "    \n",
    "    for sequence_num, (observation_sequence, label_sequence) in enumerate(sequence_data):\n",
    "        for position, label in enumerate(label_sequence):\n",
    "            \n",
    "            if skip_list and skip_list[sequence_num][position]:\n",
    "                continue \n",
    "                \n",
    "            if (not label is None) or extract_all:\n",
    "                label_list.append(__get_label(label))\n",
    "                features_list.append(__get_features(observation_sequence, position))\n",
    "                location_list.append((sequence_num, position))\n",
    "                \n",
    "    return label_list, features_list, location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9b992502-a89b-47b7-83db-100b6f9f6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_pairs = [[['Winnie', 'is', 'cute', 'the', 'cat'], ['PROPN', 'AUX', 'ADJ', 'DET', 'NOUN']],\n",
    "                  [['This', 'is', 'me'],['DET','AUX','PRON']]]\n",
    "# sequence_pairs = [[['Winnie', 'is', 'cute', 'the', 'cat'], ['PROPN', 'AUX', 'ADJ', 'DET', 'NOUN']]]\n",
    "# sequence_pairs = [[['Winnie', 'is', 'cute', 'the', 'cat'], ['PROPN',None,  'ADJ', 'DET', 'NOUN']]]\n",
    "# data_train = [[['The', 'dog', 'saw', 'the', 'cat'], ['D', 'N', 'V', 'D', 'N']]]\n",
    "# sequence_pairs = [[['The', 'dog', 'saw', 'the', 'cat'], ['D', 'N', 'V', 'D', 'N']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "15604de7-5a36-4b1f-822f-830f4425e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list, features_list, _ = extract_features(sequence_pairs, False, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d488f21b-b349-4f79-b2d2-eb359f629516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 4, 2, 6]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2dbfde7e-c44e-49f8-8d64-ccdebc17fd7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 1,\n",
       "  2: 1,\n",
       "  3: 1,\n",
       "  4: 1,\n",
       "  5: 1,\n",
       "  6: 1,\n",
       "  7: 1,\n",
       "  8: 1,\n",
       "  9: 1,\n",
       "  10: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  13: 1,\n",
       "  14: 1,\n",
       "  15: 1,\n",
       "  16: 1},\n",
       " {17: 1,\n",
       "  18: 1,\n",
       "  19: 1,\n",
       "  20: 1,\n",
       "  21: 1,\n",
       "  22: 1,\n",
       "  23: 1,\n",
       "  24: 1,\n",
       "  25: 1,\n",
       "  26: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  27: 1,\n",
       "  14: 1,\n",
       "  28: 1,\n",
       "  29: 1},\n",
       " {30: 1,\n",
       "  18: 1,\n",
       "  31: 1,\n",
       "  4: 1,\n",
       "  32: 1,\n",
       "  33: 1,\n",
       "  34: 1,\n",
       "  35: 1,\n",
       "  36: 1,\n",
       "  37: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  38: 1,\n",
       "  39: 1,\n",
       "  40: 1,\n",
       "  41: 1},\n",
       " {42: 1,\n",
       "  18: 1,\n",
       "  43: 1,\n",
       "  4: 1,\n",
       "  44: 1,\n",
       "  45: 1,\n",
       "  46: 1,\n",
       "  47: 1,\n",
       "  48: 1,\n",
       "  49: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  50: 1,\n",
       "  51: 1,\n",
       "  52: 1,\n",
       "  53: 1},\n",
       " {54: 1,\n",
       "  18: 1,\n",
       "  31: 1,\n",
       "  55: 1,\n",
       "  56: 1,\n",
       "  57: 1,\n",
       "  58: 1,\n",
       "  59: 1,\n",
       "  60: 1,\n",
       "  61: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  62: 1,\n",
       "  63: 1,\n",
       "  64: 1,\n",
       "  53: 1},\n",
       " {65: 1,\n",
       "  2: 1,\n",
       "  66: 1,\n",
       "  20: 1,\n",
       "  67: 1,\n",
       "  22: 1,\n",
       "  68: 1,\n",
       "  69: 1,\n",
       "  70: 1,\n",
       "  71: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  13: 1,\n",
       "  14: 1,\n",
       "  15: 1,\n",
       "  72: 1},\n",
       " {17: 1,\n",
       "  18: 1,\n",
       "  19: 1,\n",
       "  20: 1,\n",
       "  21: 1,\n",
       "  22: 1,\n",
       "  23: 1,\n",
       "  24: 1,\n",
       "  25: 1,\n",
       "  26: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  73: 1,\n",
       "  14: 1,\n",
       "  74: 1,\n",
       "  53: 1},\n",
       " {75: 1,\n",
       "  18: 1,\n",
       "  76: 1,\n",
       "  4: 1,\n",
       "  77: 1,\n",
       "  78: 1,\n",
       "  79: 1,\n",
       "  80: 1,\n",
       "  81: 1,\n",
       "  82: 1,\n",
       "  11: 1,\n",
       "  12: 1,\n",
       "  38: 1,\n",
       "  83: 1,\n",
       "  64: 1,\n",
       "  53: 1}]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "732f372b-14a7-486f-a6cf-a7e38b2a3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (1, 0), (1, 1), (1, 2)]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "acec7fde-d446-4e6d-9545-694ddf5e7f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c7b7657f-d992-44a9-a69e-3b4a5481fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import datetime\n",
    "import math\n",
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "path = \"liblinear-1.96/python\"\n",
    "sys.path.append(os.path.abspath(path))\n",
    "\n",
    "import liblinearutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "702b9856-1994-4c85-b4c1-64082c1d0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"{0} labeled instances (out of {1})\".format(len(label_list), data_train.num_instances))\n",
    "# print(\"{0} label types\".format(len(data_train.label_count)))\n",
    "# print(\"{0} observation types\".format(len(data_train.observation_count)))\n",
    "# print(\"\\\"{0}\\\" feature template\".format(self.__feature_extractor.feature_template))\n",
    "# print(\"{0} feature types\".format(self.__feature_extractor.num_feature_types()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "a695856f-d154-4747-b644-5c97bbccbbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 labeled instances (out of 8)\n",
      "8 label types\n",
      "5 observation types\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} labeled instances (out of {1})\".format(len(label_list), len(features_list)))\n",
    "print(\"{0} label types\".format(len(label_list)))\n",
    "print(\"{0} observation types\".format(len(sequence_pairs[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3386bbad-1ec6-49cf-8b4f-e978bae2ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = liblinearutil.problem(label_list, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1438d7fc-130d-40a9-aee3-24081c6c4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "__liblinear_model = liblinearutil.train(problem, liblinearutil.parameter(\"-q\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f34dfb14-c1a3-4edb-98ab-968328ef6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = [[['The', 'rabbit', 'ate', 'a', 'zebra'], ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']]]\n",
    "# data_dev = [[['The', 'rabbit', 'ate', 'a', 'zebra'], ['DET', '', 'VERB', 'DET', 'NOUN']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3e21f62b-b9e0-4691-a454-b8e7ab4dfd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'rabbit', 'ate', 'a', 'zebra'], ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']]]\n",
      "[4, 5, 7, 4, 5]\n",
      "[{84: 1, 2: 1, 66: 1, 4: 1, 67: 1, 45: 1, 85: 1, 86: 1, 87: 1, 88: 1, 11: 1, 12: 1, 13: 1, 14: 1, 89: 1, 90: 1}, {91: 1, 18: 1, 92: 1, 55: 1, 93: 1, 94: 1, 95: 1, 96: 1, 97: 1, 98: 1, 11: 1, 12: 1, 99: 1, 14: 1, 100: 1, 101: 1}, {102: 1, 18: 1, 103: 1, 4: 1, 104: 1, 33: 1, 105: 1, 106: 1, 107: 1, 108: 1, 11: 1, 12: 1, 109: 1, 110: 1, 111: 1, 112: 1}, {113: 1, 18: 1, 103: 1, 114: 1, 115: 1, 116: 1, 117: 1, 118: 1, 119: 1, 120: 1, 11: 1, 12: 1, 121: 1, 122: 1, 123: 1, 53: 1}, {124: 1, 18: 1, 125: 1, 114: 1, 126: 1, 127: 1, 128: 1, 129: 1, 130: 1, 131: 1, 11: 1, 12: 1, 132: 1, 133: 1, 64: 1, 53: 1}]\n",
      "[4.0, 2.0, 3.0, 4.0, 5.0]\n",
      "60.0\n",
      "[[-0.3567717329132573, -0.7695453806572491, -0.6480543293489829, 0.1442477438021597, -0.7720423985213759, -0.6605928223149127], [-0.4623311890539694, -0.3093712967727721, -0.48958499004204875, -0.38048493486315904, -0.3295724196854575, -0.5295406300247584], [-0.4754913874874138, -0.4850895732186991, -0.23460998764339464, -0.38532409974164744, -0.5458705305608019, -0.434742342929481], [-0.54574463012385, -0.34304964444796426, -0.4754449867991651, -0.3111409887090474, -0.34183984984402477, -0.37172900192052016], [-0.6095365412030204, -0.42580031315394123, -0.5723160130178583, -0.40749451927659686, -0.2759106124710203, -0.2970137844390108]]\n",
      "Prediction time: 0:00:01\n",
      "Per-instance accuracy: 60.000%\n"
     ]
    }
   ],
   "source": [
    "_, acc = predict(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "839a3889-cf1f-4f00-8995-7f393ac43a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted labels:  ['DET', 'AUX', 'ADJ', 'DET', 'NOUN']\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted labels: \", _)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "759cbd8a-9e97-4085-b8d4-6c1f17784355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['I', 'am', 'Winnie'], ['PRON', 'AUX', 'PROPN']]]\n",
      "[6, 2, 1]\n",
      "[{134: 1, 2: 1, 135: 1, 136: 1, 137: 1, 138: 1, 139: 1, 140: 1, 141: 1, 142: 1, 11: 1, 12: 1, 13: 1, 14: 1, 143: 1, 144: 1}, {145: 1, 18: 1, 103: 1, 146: 1, 147: 1, 148: 1, 149: 1, 150: 1, 151: 1, 152: 1, 11: 1, 12: 1, 153: 1, 14: 1, 154: 1, 53: 1}, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 155: 1, 156: 1, 64: 1, 53: 1}]\n",
      "[4.0, 2.0, 1.0]\n",
      "66.66666666666667\n",
      "[[-0.17144611502594054, -0.4360030982285758, -0.5238695914662893, -0.15182803422128668, -0.4978469290237178, -0.534583266292751], [-0.5363189021357877, -0.31050222032100344, -0.5857139764825223, -0.36715071773538116, -0.451599115702811, -0.4826935422431142], [0.5640609517723756, -0.9097520645855963, -0.9250022236486652, -1.0284521834273177, -0.6683649454079148, -0.6242250494900458]]\n",
      "Prediction time: 0:00:01\n",
      "Per-instance accuracy: 66.667%\n"
     ]
    }
   ],
   "source": [
    "data_dev = [[['I', 'am', 'Winnie'], ['PRON', 'AUX', 'PROPN']]]\n",
    "_, acc = predict(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1a5717ef-d6a3-4985-9c67-2bc75a014518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted labels:  ['DET', 'AUX', 'PROPN']\n",
      "66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted labels: \", _)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c63e091-831f-4218-b818-982f06917bbd",
   "metadata": {},
   "source": [
    "# About prediction (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "22b3cfb2-52ea-4b96-8396-30a3cf68799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_test):\n",
    "    # print(data_test)\n",
    "    start_time = time.time()\n",
    "    [label_list, features_list, _] = extract_features(data_test, True, [])\n",
    "    print(label_list)\n",
    "    print(features_list)\n",
    "    \n",
    "    pred_labels, (acc, _, _), _ = liblinearutil.predict(label_list, features_list, __liblinear_model, \"-q\")\n",
    "    \n",
    "    print(pred_labels)\n",
    "    print(acc)\n",
    "    print(_)\n",
    "    \n",
    "    \n",
    "    num_seconds = int(math.ceil(time.time() - start_time))\n",
    "    print(\"Prediction time: {0}\".format(str(datetime.timedelta(seconds=num_seconds))))\n",
    "            \n",
    "    print(\"Per-instance accuracy: {0:.3f}%\".format(acc))\n",
    "    \n",
    "    for i, label in enumerate(pred_labels):\n",
    "        pred_labels[i] = get_label_string(label)\n",
    "    return pred_labels, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37489a42-1fe7-4722-8d6c-74c8105d4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(label_list, features_list):\n",
    "    \"\"\"Trains Minitagger on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "  \n",
    "    problem = liblinearutil.problem(label_list, features_list)\n",
    "    __liblinear_model = \\\n",
    "        liblinearutil.train(problem, liblinearutil.parameter(\"-q\"))\n",
    "      \n",
    "    quiet = False\n",
    "    if not quiet:\n",
    "        num_seconds = int(math.ceil(time.time() - start_time))\n",
    "        print(\"Training time: {0}\".format(\n",
    "            str(datetime.timedelta(seconds=num_seconds))))\n",
    "        if features_list is not None:\n",
    "            quiet_value = quiet\n",
    "            quiet = True\n",
    "            _, acc = predict(label_list, features_list)\n",
    "            quiet = quiet_value\n",
    "            print(\"Dev accuracy: {0:.3f}%\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "0b9e2abd-66cd-4c6d-b8e2-02fbaab3de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(label_list, features_list):\n",
    "#     start_time = time.time()\n",
    "\n",
    "#         # Extract features (on all instances, labeled or unlabeled) and pass\n",
    "#         # them to liblinear for prediction.\n",
    "#     pred_labels, (acc, _, _), _ = \\\n",
    "#         liblinearutil.predict(label_list, features_list,\n",
    "#                                 __liblinear_model, \"-q\")\n",
    "#     quiet = False\n",
    "#     if not quiet:\n",
    "#         num_seconds = int(math.ceil(time.time() - start_time))\n",
    "        \n",
    "#         print(\"Prediction time: {0}\".format(str(datetime.timedelta(seconds=num_seconds))))\n",
    "\n",
    "\n",
    "#         # Convert predicted labels from integer IDs to strings.\n",
    "#     for i, label in enumerate(pred_labels):\n",
    "#         pred_labels[i] == label\n",
    "#     return pred_labels, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea21c0-a505-40ca-9193-c94a9ca90ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190c646-3004-4d08-a64d-c1b8f23a4569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a01f8936-4fa3-4598-8e72-6ff1a55b5dd0",
   "metadata": {},
   "source": [
    "# BREAKLINE \n",
    "## I am thinking to convert the dataset that we have into training testing and observe the performance\n",
    "### using the finalize data have been created (penn treebank + xyyx + andrew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "86d6f678-6b0a-418d-9af2-298e32c63b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "e0ab4dea-74b7-44cd-8f31-8d52e45c6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a40ae301-bff4-4010-b8a3-2ab42ac74b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('Its', 'DET'), ('fanciful', 'ADJ'), ('office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('Columbia', 'PROPN'), ('wo', 'VERB'), (\"n't\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[('谢谢', 'VERB'), ('卖家', 'NOUN'), ('，', 'PUNCT'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[('--', 'PUNCT'), ('In', 'ADP'), ('Britain', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[('Personal', 'ADJ'), ('computer', 'NOUN'), ('...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tagged\n",
       "0  [('Its', 'DET'), ('fanciful', 'ADJ'), ('office...\n",
       "1  [('Columbia', 'PROPN'), ('wo', 'VERB'), (\"n't\"...\n",
       "2  [('谢谢', 'VERB'), ('卖家', 'NOUN'), ('，', 'PUNCT'...\n",
       "3  [('--', 'PUNCT'), ('In', 'ADP'), ('Britain', '...\n",
       "4  [('Personal', 'ADJ'), ('computer', 'NOUN'), ('..."
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b0abd838-e1fa-46e7-91c1-5ced55640cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string2_list(text):\n",
    "    return ast.literal_eval(str(text))\n",
    "\n",
    "data = df.tagged.apply(convert_string2_list)\n",
    "data = data.to_list()\n",
    "\n",
    "# train test split \n",
    "cutoff = int(.80 * len(data))\n",
    "training_sentences = data[:cutoff]\n",
    "test_sentences = data[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "91507b6e-2bf2-417f-991e-8199ec527e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all these data are in the format of listof tuple\n",
    "# a function to change them into list of list is required \n",
    "\n",
    "def convert_listoftuples_to_listoflists(listoftuples):\n",
    "    training_data = []\n",
    "    for each_sentences in listoftuples: \n",
    "        observation_list = []\n",
    "        label_list = []\n",
    "        for i in each_sentences:\n",
    "            observation = i[0]\n",
    "            observation_list.append(observation)\n",
    "            label = i[1]\n",
    "            label_list.append(label)\n",
    "        sentence_list = [observation_list,label_list]\n",
    "        training_data.append(sentence_list)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4f9690a9-4506-49a3-8b7c-20a525c83b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = convert_listoftuples_to_listoflists(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "7db9caa9-d91c-4689-81bb-dbf92337a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = convert_listoftuples_to_listoflists(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "162aaeeb-bdd3-41eb-9ba5-b9be707cf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rese4t all these values\n",
    "__map_label_str2num = {}\n",
    "__map_label_num2str = {}\n",
    "__map_feature_str2num = {}\n",
    "__map_feature_num2str = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "1e1402d5-7341-41f8-bbc7-8e3bea6d72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list, features_list, _ = extract_features(data_train, False, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "f36feb8c-7a84-4d81-b26a-1d7924a7522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78342"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "225ae585-d440-47ef-8515-c8f9cffeeb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78342"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "4d7e0ce0-f34b-440b-b71b-13225cbb46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78342 labeled instances (out of 78342)\n",
      "78342 label types\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} labeled instances (out of {1})\".format(len(label_list), len(features_list)))\n",
    "print(\"{0} label types\".format(len(label_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9b721405-362d-4a1d-97e2-36c3de47612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = liblinearutil.problem(label_list, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "1314b66e-f02a-4659-96eb-8a0a12cdfe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "__liblinear_model = liblinearutil.train(problem, liblinearutil.parameter(\"-q\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "094eae58-85c6-42e9-996e-9822d2b7984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 0:00:01\n",
      "Per-instance accuracy: 89.972%\n"
     ]
    }
   ],
   "source": [
    "_, acc = predict(data_dev)"
   ]
  },
  {
   "attachments": {
    "3ab103d5-eb61-4ca5-af10-7306becc7165.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAABQCAYAAAC3UxTfAAAABHNCSVQICAgIfAhkiAAAIABJREFUeF7tnQm8VlP3x1dElEQK4U2aiDJF5K1EhkhKicpQ5rzmFG9ElDKFpJApswghQ4kG85AiEaHMMmbMkOHfd7//9Xz23fc8zz3nGW7Pva31+Tyfe885++zht/dZ01577yobb7zxP2KUCIH99ttPnn/++UTvWGJDwBAwBAwBQwAEVjMYDAFDwBAwBAwBQ6D8EDDBW35YW0mGgCFgCBgChoBZvDYGDAFDwBAwBAyB8kTALN7yRNvKMgQMAUPAEFjlEchZ8G611Vby4osvpn4hokcffbSMHz9ennrqKXnyySflmmuukRYtWpRItscee8iYMWPk8ccflxkzZsjtt98u+++/f5iV1K5dW8455xyZPHmyPP3003LjjTdKq1atSqU7++yzU/XhfyNDIBcEunfvLg8++GBkFqeddpqMGjUq8lmSm+3bt5c2bdrEemWnnXaSV155RWrUqFFm+osuukjOP//8MtNFJdhggw2kV69esu6660Y9Lvi9XOoep3IrAktl3rx50qBBgzjJLY0hkDcEcha8WpOTTjpJunbtWqpiCOaXX35ZrrzySrnqqqtk9dVXl7Fjx0rTpk1TaRs3biwff/yx3HDDDTJixAhZsGCBnHfeeQLDU6pSpYpcdtll0rp1a8foBg4cKF9++aXLd8sttyxR7nXXXefq8u6775aqj90wBPKJwNdffy2ff/55zlnutddesvvuu8fKh7GPMvvLL7/ESp9tIgRv3759Zb311ss2i6J+b8mSJU6JP+OMM4q6nla5yodA1Xw16bvvvnOCMKSzzjqrxK3p06fLY489Jp06dZKFCxe6Z1iuPk2bNk022mgj6datmzzwwAPu0SabbCLbbLONXHzxxc7ahebPn++YFdaCL2R//PFH4bd8+fIS+dqFIZBvBO6+++58Z5kxv+2331622247OeWUUzKms4fxEJg4caJMmDBB6tWrJ1988UW8lyyVIZAjAnkTvHHr8fvvv8uvv/5appts6dKlJVxAWLzQsmXLUkUhWP/666+4RZdKh/WNpfHbb7/JrFmzSj2vCDe6dOkie+65p/zrX/9y3gSUGayht99+u0T1eXbQQQdJx44dBRcb+D733HNy8803y59//plKi+se92KjRo0cLm+88YbcdNNNkUpVeeDD1ATeEPoaBYvxg5Vy1113yT//lFyCPnXqVDdlsc4660jnzp2di/TTTz91Qor3waBnz56y7777CtYcz2jbq6++WqIpvHvooYe691966aVIi5Z6qdcGD83pp58eCUfNmjXlyCOPlN12283lh3X88MMPu2kViDr71K5dO3dJuqOOOqrEMy7wAr322mvy1VdflXq2xhpryPHHHy8dOnSQv//+Wx566KFSadZaay1nxeKuRrmlj8kPHL755huXnv4njZKvGKP4zpw50z3adNNN5fDDD5fmzZu7aSDGlE4VJVV649SdMrG+mb7addddZe2115a33nrLedA++eQTVyfaj8sezH3q3bu3U+TpV59nML6xfPGQ4SkzMgTKA4FyEbwwvDXXXNMxxIMPPth9PFi9IfEhVatWTXbeeWfHPBAKSjDJOXPmOKaAdYtFy4fEB/7oo4+GWcW6Xm211eSCCy5wTKyiCl7c9Myff/bZZ46hIJxwycOcfOaM8Nlnn32cds+8FowSIQzmP/30k8MLAc6cOMwTS65q1aqOwW2xxRaxBW/D5a2k0+JBafH/sOZsmVxveNrnUQ8YC3feeadj8rSXPoPJR40hFJEPP/xQLrzwQqfg/fvf/xZV2gYMGOCY8m233Sbvv/++m7YgHdMkixcvdkXvsssucvLJJ8ukSZPcFAnxB4wzXzkh3bnnnisIi2OPPdZZS1HEWL788ssd1rfeeqvro/r16zuhp4IXgQDhGUKpuPrqq911OoWS+j/yyCNRxTlhyeYu48aNc9bbYYcd5hSo2bNnp9IjeOvUqSP33HOPGx/0/yGHHOI8SQgtlBkENvEYDRs2FOZZcW1Td+jnn39O5UW7uaY8+gNBzrgjTxSgJBS37ldccYXjJQhJxi2KCHWn3D/++EOeeeYZd49xQh8rodCw6U2IK+2Fr7Rt29YEb5IOs7Q5IVAugheN+Prrr3cVxWKFycydO7dUxXFDK/ExwyB9gnHCCO677z53G/c2AkWZQqkMV4EbMCKf0OCxrmDQGhCENQxDpg8QKEoIWBVKKEcwXiy8Sy65JJWGwDkUlLhU7e8aUlcapk2+9KdPRaLlVNp3mEdVC/f11193AhfLJUrw0h7iBLD4IKxlCEaMYjF69OjUe+SFcCEvbXOPHj2cFaXjlXHarFkzZyH7hOIHYTGmIxQGlBYEuwoB+sev97fffuteR4FE8Op1VJ7UgSmXqNgFBOoBBxzgvg08AhDKxB133FEiq++//959Qz4hpLF4EdLUE4WFn7aZd6LqhUD3hTq4UQ/GEVZo6JGIahP34tadMUz7TzzxRKdcQXh2UBLB+oknnnDYMF4Y/4o5CgL9QAxJFJEHSinjXMdNVDq7ZwjkC4FyEbzvvfeeHHfccc7SZW532LBhTmDiovOJNHyEuDuPOeYY+eGHH1KCAoY6ePBg51LF2oAZHHjggS7Qig9RLZYkwMDssHoqMm2++ebOEsTtuf766zvmgaXK/0rMCUIIWp9gjMoccRvyDtZzSEmY0YJqM+T8pv8rL8wn22vc5z4TZzzhOcGD4ltg5I/7PKq+igEWkU8IXwSyEgJaFTu9h3DBPZ2UdtxxR+cC9S0v8oiqX5y8sVQhxn5I9B/fjq/QIiw/+uijMKn7vlAwNttsM4ehKl9Jg6gYa3yDe++9t7N2KZ97KHFY+5mUEr9ScevesmVLJ3BV6JIHijzjAaUBYpzgvSL2Ay8DhDWLokRfRxEKPN4LpgXgOUaGQKERKBfBy8dBIBT0wgsvOO361FNPdQLTJ00Do+MDZqnGlClTnPaNew4GecIJJzhXKYT1gLaLwGaZ0apGCMqRI0fKBx984CLGYbQwdaw330qFocCQMjEVXTICEyo2CqN39bpWrVqlBK/OU4ZtUKGCi9UncNL5SJgvblK1ZjVdeB3mne4aTKMsxXTpy7qPMINwqYYEFlDYx+H1DjvsIEOHDnXuar5DFBewYXUAAjMJMY+KWxdLEqsRQYurnm80SV5x6046lv6EU0uUBY9QQvCimGHlopC3XzH98uyzz6ZVePA0QCgOIV5J8LC0hkBcBMpF8PqVQTDgDsI1lIneeecdpzXjJlq0aFFKo/U1eOZr/GeZ8quMz7CoWMup85naxnDdJXNhWDUwWObiokiFC/ORuVCjP3eRzosGp81icfVX5eHNhqZ9HvUgtMS0fVFMMp01SVoEbL9+/Uq5QPUdnqMkVq9evUQ14qyXjao3mML880Xa3rB/yV+fRdXdD0hkzhoPwrXXXpuqFspGNoQizNSFurbJQ63nJPnFrTvpUM6j1k371jXtY/4adzNtxxpO52amnjq+osZTknZYWkMgLgLxJ+/i5uilw+UZfojcw+3nWwJYGiEhVCAVFGrJqEuJZ2i6XGe7DID32ajDdzWG9SjkNUFkw4cPF41kTVoWignCwmc6zKejufukHgKYrk/0jVrGzJODNW7DkJJYL2v+VV1qS/20v5rL6obZl3lNmwioUcLlCGMN3cyZMsLNyDjDaqKt/s8fP7iFw3Xh4XWmcvxnBO3gzsV97VMUnljxZQlAIp2xdpnnDIn2YPU1adIk9YhxQDCXT9wLXdWMwyhSKzIcT5o2ykLE4k1KceuOG52pFRSasA9DzwKR13xXCF8Eqn4DUXVj+gps47rGo/Kwe4ZAEgQKavESuIJgYV0uUckwFiJpsQL8AA/W0uEKwmWKIIERMKdGsIQKXoJ+CJpgYw0Cr/iYmC/mo2FpRzaE0CE/mLgf2JVNXtm8g9KASwwrPpx7jJPfm2++6YQJy4SwOmCyuPBDBkKAEctWiMDFWuI9/iJkiQjFIsZ7wLIRAt8GDRrk5oMRzChALDcB/zhUiDleBBX9xDIcBAuKEsE7SQiBSpuGDBniInpxQeKCZ224tp387r//fpeGeUHazVwhgt93ZSJw1OpkTNMHG264oasO6TRKnPXmRFkT7MVubAgLPDhgGgY44bkhCpkocr4VvoNwXTz3UCC23XbbUk2nzzXojNgJ3iVOQt3T+gLKAFHb7B6H54m6sHwqivguaA9j9N5773XjiqkIxQJ8CHhiXh0liHz8jXGi8oy6F7fuRILzzRMpDs9AGWfeG2HPFJYuc6IM3M1Ea+MK59tK5wkhLXiGS8qi6mn3DIF8IVBQwcuHy8cJoyT4Am0dhnfmmWe6D0UJhsGyD4QyFjGMhzlLf5s+PmzmhPkRmAXzI8gCIYHQroiE0gCxbCUbInAHpt53xTISllPA2G9dEVDC/yGxTAUBDKMksAYGCm6+QEFQYHmxZIs5cxgi8+gIhZVJMFHm4QiuYwwhOKMimsuqIwwbZoyAYC0zlh9TGgh0JfqCpSrMU+KCRNAxH+p7ArCkGMM+afQwglvXvVJnIvH79OkjLBtCWNNHUetrUZxQSPv37++s8nTreGk3sQ8oI+HSGPoeQctyMpQmAuXCgCKWCdWtW9d9NwRW0X7SX3rppaXgo/7cZzwRDc636a/jxV1NxLa6rYmAx6VL/ZJSnLozVtllivFOXAeKEzyGNtIOn1C08GSg6GRSaomTYFOSTK7opG2x9IZAWQhUWcGASu5CUNYbwXO2hGTDBpi1H22YMJuCJIcB8gFGMZVcCkR4sSYwV2JXLpg/lolRNAJ4M+hDXd8anWrVuYuwQZCgfGJpGuWGABuqoMxjHISKTG4529uGQHoE8jbHiyutWDahwBqhLrgRi5VgoFtvvbWzJIwMgbgI4MZG0cXiM8oNAbwG7A7GZh8mdHPD0t5OhkDOrmbmZXFdFhPdcsstbk4KCpeiFEs9YaBszGBkCCRFgPHNfC/R1sU6vpO2aWWkZ34Yr1O6k6dWRp2szFUDgZxdzasGTCVbGeVqJiiGgJxMRDQmgWFGhsCqgIB9E6tCL1sbs0HABG8WqEUJ3iyysVcMAUPAEDAEVkEE8jbHuwpiZ002BAwBQ8AQMAQSI2CCNzFk9oIhYAgYAoaAIZA9AiZ4s8fO3jQEDAFDwBAwBBIjYII3MWT2giFgCBgChoAhkD0CJnizx87eNAQMAUPAEDAEEiNggjcxZPaCIWAIGAKGgCGQPQI5b6CRfdEV+032ljYyBAwBQ8AQMASSImDreJMiZukNAUPAEDAEDIEcEDBXcw7g2auGgCFgCBgChkBSBEzwJkXM0hsChoAhYAgYAjkgYII3B/DsVUPAEDAEDAFDICkCJniTImbpDQFDwBAwBAyBHBAwwZsDePaqIWAIGAKGgCGQFAETvEkRs/SGgCFgCBgChkAOCJjgzQE8e9UQMAQMAUPAEEiKgAnepIhZekPAEDAEDAFDIAcETPDmAJ69aggYAoaAIWAIJEXABG9SxMpI/8UXX4j+PvzwQ5kxY4b85z//kTXXXLOMN/P3+M4775SbbropMsPnn39ezj777MhnSW526dJF9ttvvzJfueyyy2Ty5MllpivPBCeffLK0aNGizCKLse5lVbpZs2YyadIkYey9/PLLctRRR5X1StrnnTt3lmeeecblNXXqVNltt91KpV1//fVl7Nix8t5778mCBQtkxIgRstZaa5VKF+dGnLrTnrvuuksWLlzovrPtttsuTtaWxhAoKgRM8BagOx555BE54ogjBAY/d+5cOe+882Tw4MEFKCl5lh988IF89dVXyV8M3ujRo4d07dq1zHw+//xzWbx4cZnpyjPBgAEDZPvtty+zyGKse6ZK165dWyZOnOiSIKD4H0FIXyWl1q1by7hx4+TFF1+UI488Uj799FO5++67pXHjxiWyQsEj7RlnnCFDhgxxZV1yySVJi5O4de/bt69Uq1ZNZs2albgMe8EQKBYE7JCEAvTEokWL5KmnnnI5P/roo7LuuuvK0UcfLcOGDZPly5cXoMT4WcJEy5NGjRpVnsXltayKVvfDDz9catWqJccee6x8++23ztvSvHlz6d+/f0ogxwXo9NNPl7ffflv++9//yj///OOs59mzZ0u/fv0ExQXaaaednBXcp08fefLJJ909xvrQoUPl8ssvl88++yxucRK37nvuuaf89ddfsscee8gBBxwQO39LaAgUEwJm8ZZDb7zyyiuy+uqryyabbJIqrVWrVs5V1rJlSxk9erRznWGNXnHFFak0G2ywgVx55ZUyf/585+6bMGGCNGrUqESNcWFj1bz77rvO1QeTDal69eop9zdlZnI1N2nSRG6++WZ555135P3335cHHnhAdt1111SW6kbv0KGDHHjggal8cWH79Pjjj6eepXM1V6lSRU488UTH1D/++GN59tlnpWfPniXyOfPMM+W1114T3J7PPfecwwhXOtgkIcWb+mMx4UbWtvB/krpjCd56660yZcoUmTdvnuy7777OnQ7+Yf0RfPfcc4+gjNFH9O8666xTquqMj+7du0vHjh1LPYt7Y/fdd5c5c+Y4oatEPzRo0EDq169fKpv27du7MhkfPjGmsGJxLyN0od9//12mT5/uBJ5S27ZtnRD0rU8EMP0adXpXuvLIL27dKc/IEKjoCJjgLYcehPFBX3/9danSRo4c6YQq2jtMEAEMwQwfeughgbnhpsbFBhO87777nOBQwiLp3bu3E77MJcPcQqb366+/Otcqv48++qhUHfQGQv2JJ55wCgJWDe5KhF27du1S72g+CEmYrF4jhH3CguHZ/fffn7Y8rCfc8LSpV69eMnPmTLnqqqtKubDr1Knj7h1yyCFOMDVt2lQGDRqUNt+oB6+//nqqrgiR4cOHp64vuuiixHWnbXgwcKOPHz9e7rjjDqcY+VMKW265pRPIVatWdVYoSkSbNm1KKFdaMIJ3zJgxrl7ZEm5gFBNIham6+bfYYotS2VIfygRfnzbddFNZY401UlMENWrUcI9RHhgba6+9trumPNzx4MmYpJ24pBGOoYJI+nTlaV5J6u4qYGQIVFAEzNVcgI7DYqhZs6ZjRghCBCNuv2XLlpUq7dVXX3VWrRICAuKdhg0bCq41LCUIVx9zxt26dXNWFMwVgQzzvO2221yat956y1k9PiGwv/zyS3crk8UwcOBA+eWXX1z+CGsIAQtDVdJ8/vjjD/ntt99S+aYS/P8/3333nftP8wmfYxWhKCCY1crHat56663dfZQOJfA855xzUmUh4A477LAwy4zX1FfrTsIffvgh67rzPnXlh0W51VZbOeVhm222ca5YAo6WLl3qlINvvvnGzfeDFYTyRdsuvfRSJ8jySbiZf/rpJ0HgT5s2zY0L4g0gXMBxibEL/fjjj24cohxiyXMNYbHTr+RJeYyPF154wbmWUcAYQ9QlCeWr7knKtLSGwMpCwCzeAiCP4MByffPNN+Waa65xTIngkyh67LHHom47gc0cmwpdEv3888/Otbntttu6d7BisD4QjkoETuEmzoawrqlPKCz//PPPbLLL+A6WE5ZWGCSDgoIA86PAv//++xJCEgG64YYbZsy/0A9VCNEnCHGI/yEUIhQL3KfM8avQ5Rluc5QfFAyfUAzq1avnph5yJfJCIFIv6pGOcN9TJm7+dIQQpV3hmPDTo9hRFsoGlK7MOOXFrXu6+tp9Q6AiIGAWbwF6CSsO1yMMF9ebWn9RRS1ZsiTqtovyhDnjhvYJF6AyeNJAYf7+HF9k5mluYqn5VmGaZHm5rVaVMmvNlOvVVltNcG/ChKHQU/D333+XsMLzUqGEmejcJ3XR//UvbmMUIpbV4GIOl/TwHIGXb0L4gSvuZZQXaOedd3Z/VVGIUyZCG8KiZSw//PDD7lqXE+n4I0+saxQJPDMQfYfiocqIuxmD8lX3GEVZEkNgpSNggrcAXYCwJaAqDsG4owgB9NJLL6UiSP00KohUwKoQ0zTqAozKN9M9ytx4440zJcnbM2XuCHufUCbABEurIhMWInOfN954o1uGE1KoLIXPs7kmGC6cW9W53SRLunAZ4+UI54WZ+mBOV61fymM9N1MqtBXabLPNXCChztfGbUe+6h63PEtnCKxMBMzVvDLRz1A2LlisCRg0TNP/qVWKNYwQVtcz2WFt8F42hMt6//33TwXPaB4w0pAQnBp0Ez6Lcw0DZ/7Tj5LlPSwn5qnV2o2TV9I0WGq51D1OeVi/bD5B34T9x3VoEYIxwWO5LJFhzOywww4lgqXoTwLqotzJYE2ZIRZgz/QI0drqNka40ldMBSgxXqg3LnWlffbZx3kAwih3nqcrj2dJ654q0P4xBCogAmbxFmmnsTsPa24ffPBBue6661zgCtGme+21l4s8xv2H0CWo6pRTTnGBV6QhCCncOQirUqNcCYTBIiYvCKtZ5yAJcmKJDME/1157rTC3ys5AuLf9ZU68h3AkShVGi7UCs8bShyhLLVmYOvO1Wp7OicKcKYOoZoQC1j07YbF0iSVGhSTqfuihhwqBbSg2CGL1HsSpe9y6sZEEwVcszyL4iv7CaiQAiXWyn3zySSorBNjVV1/tLErmhbMhllkdf/zxbtcyYgtYZ4vwPPXUUyOzI+6ANGAfehioC25mllox3ohSZ9xcf/31qbwYc2ywQaAYrnV+LFWjrVFreDOVF7fuRJNvtNFGqR2rcH/jpUG5yDa2IRIcu2kIFBABE7wFBDeXrGGEMGiWCyGc1ltvPcfMWN6jkc/kD3OH4bHeFusEZoml5dMFF1zgLBslNvPgBxFxq5t9sO0fFhLME4aLkH7jjTccYw2JdazMQRORzZpaInR1GRNWG4zbJ5g0hBLBBgsQTJw6910Rmc0uXzBPmLMf0VwikzxdnH/++a599957r7P2mI8/66yzXO5x6x6nKgTHderUyeGJIARPcGIZVrbz8JnKRYmgn1lahrAngvrcc89NvHkGZWDxnnDCCQ4X8iTIj0hylCyfmMNmORaKGe5pxp/2b6a6hs/i1p1xAqZK9CWES1//D/O2a0Og2BCoskJb/N8K+WKrmdXHEDAEDAFDwBCohAjYHG8l7FRrkiFgCBgChkDxImCCt3j7xmpmCBgChoAhUAkRMMFbCTvVmmQIGAKGgCFQvAiY4C3evrGaGQKGgCFgCFRCBEzwVsJOtSYZAoaAIWAIFC8CJniLt2+sZoaAIWAIGAKVEAETvJWwU61JhoAhYAgYAsWLgAnePPeNHq7OX7Z0ZIs9TivyT9vJc5GJsmMrPzZ0KA/acccd3U5KRoZAvhBo3ry5OxKTXao4/euWW25xu4H5xLd24YUXus1f+AY5cat169ZZVYEd19iMhI1D2GCG8nQXNjLkSEj/m/f/9zcboY5sdsMGOGwZynabp512WtHwhazAsZeyRsB2rsoauvQvcgbqxIkT3daN7E/LzlNsa1cMO+uweT1HB5YHtWrVyu1IdcMNN5RHcVZGJUeAoyDZGYtdtNhVi61MOfOYb439ovXUJL4ztrhkBy2EHKdDIazZbjXceSsTZJy0dPvtt7vzh9nik1OY2AmMLTHZt5ptUtlNjt27Qrr44ovddppKHImI8OdddmhjD+8BAwa43d9oC8Q+28OGDXMnk7GbGru8GVVOBEzwFqBf2RZQt2Fk3132uGWLRj6q5cuXF6DE+Fmy/7ORIVAREeCM6lq1arltRfnGILZWZT9zvCtslYqQ7NOnj9uyFOsUwsvDvtx4X3Rr0Djtb9eunROM3bt3d1toQljQ06dPd/uKs186h4WEZ2o3bdpU6tat65QEJQQu24bqaWS8i2Dv16+fDBkyxB2awTasKAtY1mPHjnV/lY/Eqa+lqTgImKu5HPqKIwLZBJ/D35XUZQYDQYNnr1uYRki4rmAkuKXmzp3rPvynn346kYuKjf99F1iUq5l9hEmD9s0eyljGMKuePXuGVZI2bdq4jfxJw4EDWBwcpqDENXnBUNjH2S+bDe6VYKQcmacuQRhaWB4HMXB4PBYDbjrKZEN98g2pSZMmzi2IGxLLhv2rOXTBJ/KZNm2aw5F8TzrppLQHt4f5+9coUjBc+o4+pCwOHAiJfgdThAJlMhbYOxtrzacOHToInhIsNFyo48aNK+HSnDRpUqn9r2HOYO0TXgbwbtmypYwePVoWLlzoMPMPuYhbd/YHZx/mOXPmpAQO+zUrIfCwCENC6M2cOTO8LfQ3QkwP7CiVoIwbekqWf6BDeFZzgwYN3J7Yujc4WWKZgjvjNgk1btzYJff3Rl+wYIETkpnyYm9rDhjxMaBPwiNA58+f7/JnLHOcI6d1MY7mzZvnDj/J1j2epI2WduUgYIK3HHCHGUBsWg9xbN/kyZMdg8BNhXDhQw5PANKqHXPMMU7zhmHuvffejkHrcW2aJtNfzk/lVBd+uLkyESfZwOQ5QB33HHXijFUlBCeCD4GDkESo4FLzD3bnHmVxgAIMSMvmL8xFiTbBEPv37+824qc8Di8Ij8arU6eOdO3a1aXp2LGjYFHgYvQJxsUpOig3uPBwLyKosVqUqC9ub5gyAmTMmDEuLQdFJKUWLVo4oUf/kRfCDQvHn/8jT+b1Bg8e7PqMdLggOaTeV7IOOugghymnFdHXjAdOTNLD7JPWjfQjR450whIsEXYIYKU4dWeaBCHAQR2MgV69ejlhoAfekxcnaHFN/yghVFFueBYS7QJzP32YJtP11KlT3fgZOHCgy4O+xvpFgKlFqgpN6FlC+NavX98pwHFJ4zI4/MEnzh7WbzrMi/x79OjhDvoI6xCmRfnmZDCULU72Yuwwtjn0hFO/sHiVaGu2CktYrl2vfATM1VyAPuCD5XB6zjBFy+/du7cLslLtHKEBA4Hh65F8CGU+Vk4CUjeaVg0hy1F5qjGHJ/+U1QSO4NMzfJmnykRYcTA4CCaJUOToNVxlEMKSdiEgNU8EnE96yDtuOMqBXoDZAAAIVklEQVTTdCUSrbjgWECfEMK4DGFc/tF44Mlxh5rPhAkTnBDzCWaMJdStW7fUQe0EsKDcQOTBXDsCUIU2Lkh1XTKnBk5xCYbvE8oHR/BxuhMn5UBYTMw1Ui5H9SnRz6o4waixgMEc61uJE4y07n45cf/HW4Hio+RbbXHqfvDBB7u+RtFTy4w2+gKVIw9RLFCKtH24YBF+CO18E+MKRQAlBa8AhDXOEY8qHPXcYQSYngkM1sypgjUKRXgEYrp6qpKKUoelC6F44kbGgo2itm3bis5FRz3Xe5tvvrk7lYtvAL7Aj3GJ8gh+YItyTntRehi/uMlR8IwqPgJm8RagD4lixsKAOTCvgzauzA4mQCAIgkWFLlXA7YmQgtmFxEcYuqn8NHyoaMP6y4Vh4zpWwkqA2cFolHDhwuSGDx/urEnckdkSQSQIHdyw5IuFhrXkl0feWM2+8OZ/mJtPMDyUBqx7n5QhN2vWzAWt+PNupOMsWgLfolzXJTIKLmDsuIN5H4uFulNvv14oLBAeBJ8Q8NqfRLvyTugyJn1oaQVVyHgZzjv6iePUHTyxuFTo6vt+ncAadzOCQQmvBEqDKl9+ufQtnhEVjhkbEPGQ/sPqBmu8F8QrIEzxlKg1yHGLTCUQ1Mf0B+dCY2nrNE8S5YqpDxRiIqSxRqk7ngTane57RAmgfSgE6Yi6oqiA7ahRo1LJGCd4w8AT/BC89BVHg+K1MKGbDtGKd98EbwH6DObepUsXZwHhLsTiVcGBGwmtGxclDER/CB6YiO+y1aql0671uWrCfJj8fEaYtHmh4IJR+YIcdyjtwaJnfo8zZxEaUfUuq+zx48c7jLDysZSIFEUhCRWHcB4PphemgcGms6ypB0wbguH5uHMmL4TwjUu4OWGSWMswdaJlqTvuQt+VSZ3AL0oIaVmkgfIdab5kyZLI5sStO3ily8PPmDl6lEUUGzBEYHOvEERwFOPsuOOOk1mzZjkBy5QC1qx/3jSWI3hOmTLFjU/ctgRa4SIOx3emepIWTxPCj+kJhCnzu3h4UAZD4ptgHPM9pBPwjFsUNg0C893RtIHxz/eLso0XB48ZHh4UCaPKg4C5mgvQlzBg3KZRxMcMA8AdGcWgoph0We5hoiFxUSlla1FE1TfqHi5cfri/sHrR2gmkIkIzLmFBEAgEk/GDULKdx1q6dGlG4clziDoSfBUSy0LiEh4LhC7zsb7bMpy7pEw8HFjT/ty2X47Wyw86i6oHlmY4r58Jq3QWWdy6Mw6jvC9h3QiMA0+mB2gjFiJCsRCEcOfb8hUxlFKmNLAUlehLBCBjDG8QLmOmRhDC6QRiuvririZoDtcwfY1yh+BFMIbEfDrTMHgB0hFKJrEOpA2/dbxk4Eh7UNDxpuARQinHc8ZflAmjio+AWbzl3Id8+LhW0dJxUYY/NOqkBKNBG9dfOiafNN+y0uOKZrkDPz8AS9+DgaDZhwKD5zAWCNegUo0aNUpFIacelvEPigDzq5qvJlcLFKYLtlgvIeZc05a4RBmk9wUASkRYtq7jJHjKJ/BgKQnEfD7WGQw3JN96RqD5wpn3EURJKW7dwZM5agKxfPLrpPeZH2dOGCUKay+di5xALKw6+jkbQugRIOUHpiFcsTSjFCfu4d3AlU/9mN8PCUypE/2XjmgPniQ8AMx5Mz3gxyDoe7SfufQwRkOfs5oATxjpooIceZdvBkLQY1XjWWHKinntTJHU6epu94sTAbN4V0K/EJCCxszSF4KWYOB8zARSsFAfd24+CXemWke4ulhXrNG3CD5/rrmsctHUsSYIBsLaYOcemAlLV0IiIAULgDlv5v1gYLSNvzBELHMCinCpwYxZuhIlpMN8o66JvMUaIHCJgBWYFnN8WDw8Q1CyRhLsaT+CBSFCZClzsczNxSUEKvkyXcCcIwIKiyp0YzJHileDDR3oAywYXLiUBSZYu3gzWN+NRcOGCVhLYICLEU8AuEH8Tztw5TIPT4Q7AiXpWIlbd6ZLsOgJpmJeE2FCJC8ejnA3MjDHMmOOm3qlI+IcsB7BIW6Ak58XWDLNgYeH6HT6gO+FjTP8eXTGKBYwChVeCFzTWMpRS59QxAhW5DuM8lJRZ/JHmWWs03awwdL3CUHJSgCCAKMIxYq6Mr3Ct+dHv6Mw64oHfZd+pe4oK1i6KG+MNaPKgYAJ3pXQj1hfnTp1cls3wnARhjA2mKxvAearajBFfw4M5qgMksjqJIv0YQJYTQScIEwQvrSBX0gwFAQSZcGQsNJws2G5IHxZn8zyGtYt4naDMcKAWIOalBByWLxgSplgCnPEtacE40YgI+wRKghKlIMw4KqssrF+YMDMJbKTEQyedvB/SMy/gxkCg3KxbrGWdJcl0lM+y4dYyoVAQSjh4vQD3RAs7GzEFAVTFQgfFI0wEC0sP7yOW3eUMSLEqT/Ch76mnRqx7edL3yG0sET9JTBh2bleYxGitLAEjPGGMkUfs8bdt3hRaPi+UGaxIJkLHjFiRAkPRdy68D7L47CMGesIadZPh4RgRYmKsqpJS99BzEnz8ynqG2QMoJxRb6YqGLtJx2lYR7suHgSqrAiIiL+GonjqbTUxBAyBIkGAyHY2d2GeP8qqLJJqWjUMgaJBwCzeoukKq4ghULEQwGWPVUnkL9G54bKpitUaq60hUH4IWHBV+WFtJRkClQoB5puJpmf6gPluDQyqVI20xhgCBUDAXM0FANWyNAQMAUPAEDAE0iFgFm86ZOy+IWAIGAKGgCFQAARM8BYAVMvSEDAEDAFDwBBIh4AJ3nTI2H1DwBAwBAwBQ6AACJjgLQColqUhYAgYAoaAIZAOARO86ZCx+4aAIWAIGAKGQAEQ+D/uO7Kn3tACWwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "8392031b-ad11-4fa8-a9e0-d676b1049e2c",
   "metadata": {},
   "source": [
    "![image.png](attachment:3ab103d5-eb61-4ca5-af10-7306becc7165.png)\n",
    "\n",
    "### using baselines feature and baseline training\n",
    "## should do some ammendment in the code so that can get all the performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "25f558db-8e9d-431c-bf03-470109ca0467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109879 labeled instances (out of 109879)\n",
      "109879 label types\n",
      "Prediction time: 0:00:01\n",
      "Per-instance accuracy: 82.617%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_latest.csv\")\n",
    "df.head()\n",
    "def convert_string2_list(text):\n",
    "    return ast.literal_eval(str(text))\n",
    "\n",
    "data = df.tagged.apply(convert_string2_list)\n",
    "data = data.to_list()\n",
    "\n",
    "# train test split \n",
    "cutoff = int(.80 * len(data))\n",
    "training_sentences = data[:cutoff]\n",
    "test_sentences = data[cutoff:]\n",
    "\n",
    "# all these data are in the format of listof tuple\n",
    "# a function to change them into list of list is required \n",
    "\n",
    "def convert_listoftuples_to_listoflists(listoftuples):\n",
    "    training_data = []\n",
    "    for each_sentences in listoftuples: \n",
    "        observation_list = []\n",
    "        label_list = []\n",
    "        for i in each_sentences:\n",
    "            observation = i[0]\n",
    "            observation_list.append(observation)\n",
    "            label = i[1]\n",
    "            label_list.append(label)\n",
    "        sentence_list = [observation_list,label_list]\n",
    "        training_data.append(sentence_list)\n",
    "    return training_data\n",
    "\n",
    "data_train = convert_listoftuples_to_listoflists(training_sentences)\n",
    "data_dev = convert_listoftuples_to_listoflists(test_sentences)\n",
    "\n",
    "# rese4t all these values\n",
    "__map_label_str2num = {}\n",
    "__map_label_num2str = {}\n",
    "__map_feature_str2num = {}\n",
    "__map_feature_num2str = {}\n",
    "\n",
    "label_list, features_list, _ = extract_features(data_train, False, [])\n",
    "\n",
    "print(\"{0} labeled instances (out of {1})\".format(len(label_list), len(features_list)))\n",
    "print(\"{0} label types\".format(len(label_list)))\n",
    "\n",
    "problem = liblinearutil.problem(label_list, features_list)\n",
    "\n",
    "__liblinear_model = liblinearutil.train(problem, liblinearutil.parameter(\"-q\"))\n",
    "\n",
    "_, acc = predict(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "03c73512-11c5-49c4-bea3-9b44628e00f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79881 labeled instances (out of 79881)\n",
      "79881 label types\n",
      "Prediction time: 0:00:02\n",
      "Per-instance accuracy: 96.196%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train_latest.csv\")\n",
    "df.head()\n",
    "def convert_string2_list(text):\n",
    "    return ast.literal_eval(str(text))\n",
    "\n",
    "data = df.tagged.apply(convert_string2_list)\n",
    "data = data.to_list()\n",
    "\n",
    "# train test split \n",
    "cutoff = int(.80 * len(data))\n",
    "training_sentences = data[:cutoff]\n",
    "test_sentences = data[cutoff:]\n",
    "\n",
    "# all these data are in the format of listof tuple\n",
    "# a function to change them into list of list is required \n",
    "\n",
    "def convert_listoftuples_to_listoflists(listoftuples):\n",
    "    training_data = []\n",
    "    for each_sentences in listoftuples: \n",
    "        observation_list = []\n",
    "        label_list = []\n",
    "        for i in each_sentences:\n",
    "            observation = i[0]\n",
    "            observation_list.append(observation)\n",
    "            label = i[1]\n",
    "            label_list.append(label)\n",
    "        sentence_list = [observation_list,label_list]\n",
    "        training_data.append(sentence_list)\n",
    "    return training_data\n",
    "\n",
    "data_train = convert_listoftuples_to_listoflists(training_sentences)\n",
    "data_dev = convert_listoftuples_to_listoflists(test_sentences)\n",
    "\n",
    "# rese4t all these values\n",
    "__map_label_str2num = {}\n",
    "__map_label_num2str = {}\n",
    "__map_feature_str2num = {}\n",
    "__map_feature_num2str = {}\n",
    "\n",
    "label_list, features_list, _ = extract_features(data_train, False, [])\n",
    "\n",
    "print(\"{0} labeled instances (out of {1})\".format(len(label_list), len(features_list)))\n",
    "print(\"{0} label types\".format(len(label_list)))\n",
    "\n",
    "problem = liblinearutil.problem(label_list, features_list)\n",
    "\n",
    "__liblinear_model = liblinearutil.train(problem, liblinearutil.parameter(\"-q\"))\n",
    "\n",
    "_, acc = predict(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "12efabb4-47b2-4390-84d4-6b6e0b739573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 0:00:01\n",
      "Per-instance accuracy: 82.177%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"test_latest.csv\")\n",
    "\n",
    "def convert_string2_list(text):\n",
    "    return ast.literal_eval(str(text))\n",
    "\n",
    "data = df.tagged.apply(convert_string2_list)\n",
    "test_sentences = data.to_list()\n",
    "\n",
    "data_test = convert_listoftuples_to_listoflists(test_sentences)\n",
    "\n",
    "_, acc = predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e559f3-df55-48dc-8492-9bd20734c853",
   "metadata": {},
   "source": [
    "# Testing on new sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e577cde0-a25e-4ae9-b399-1fbfbaace6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [\"She sells seashells on the seashore.\", \"I love planting.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "7f329a5c-3c67-4467-8cea-509fdca7d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def convert_data(sentences):\n",
    "    \"\"\"convert sentence into the list of list of list format\"\"\"\n",
    "    sentences_list = word_tokenize(sentences)\n",
    "    return sentences_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "ca983733-7fbe-4606-9c61-085d56eafcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = [[['The', 'rabbit', 'ate', 'a', 'zebra'], ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']]]\n",
    "# data_dev = [[['The', 'rabbit', 'ate', 'a', 'zebra'], ['DET', '', 'VERB', 'DET', 'NOUN']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "5f7f8dbc-ada9-4feb-9bd9-e163d6a41601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(data_test):\n",
    "\n",
    "    [label_list, features_list, _] = extract_features(data_test, True, [])\n",
    "\n",
    "    pred_labels, (acc, _, _), _ = liblinearutil.predict(label_list, features_list, __liblinear_model, \"-q\")\n",
    "    \n",
    "    for i, label in enumerate(pred_labels):\n",
    "        pred_labels[i] = get_label_string(label)\n",
    "    return pred_labels, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "8a31166a-5de3-4824-b768-c8670901bd16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [375]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhappy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [374]\u001b[0m, in \u001b[0;36mpredict_output\u001b[0;34m(data_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_output\u001b[39m(data_test):\n\u001b[0;32m----> 3\u001b[0m     [label_list, features_list, _] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     pred_labels, (acc, _, _), _ \u001b[38;5;241m=\u001b[39m liblinearutil\u001b[38;5;241m.\u001b[39mpredict(label_list, features_list, __liblinear_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-q\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pred_labels):\n",
      "Input \u001b[0;32mIn [367]\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(sequence_data, extract_all, skip_list)\u001b[0m\n\u001b[1;32m     27\u001b[0m features_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m location_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence_num, (observation_sequence, label_sequence) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequence_data):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m position, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_sequence):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m skip_list \u001b[38;5;129;01mand\u001b[39;00m skip_list[sequence_num][position]:\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "predict_output([['I', 'am', 'happy'],[None, None, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f6f23-229d-4f54-b81a-a2eb0212ac36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab8b13-75b7-427a-84be-28df8fc730d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0d751-97db-498f-a53c-778828f1b9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c1c3f-2eef-44cc-a55f-c5a5e327e8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e222b71-b573-41fd-9cb2-415bb12f941b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948f115-8b86-4350-81c7-57a168ddb2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885664e-fbea-43e5-8c54-e67e74496a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "206e2317-2927-4e0d-a56f-011e5c375301",
   "metadata": {},
   "source": [
    "# ABOUT ACTIVE TRAINING \n",
    "\n",
    "It seems like the author doing margin-based active learning, least confidence consider as margin based?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "b5560a8d-03bd-4462-9320-6e12c187339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [[['I', 'am', 'Winnie'], ['PRON', 'AUX', 'PROPN']]]\n",
    "\n",
    "__skip_extraction = []\n",
    "for _, label_sequence in data_train:\n",
    "    __skip_extraction.append([False for _ in label_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "530b111a-3b91-4dc9-b3b8-b5b529d70740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __make_data_from_locations(data_train, locations):\n",
    "    \"\"\"The locations value will be something like [(0,0), (0,2)] and so on\"\"\"\n",
    "    \n",
    "    selected_positions = collections.defaultdict(list)\n",
    "    \n",
    "    for (sequence_num, position) in locations: \n",
    "        selected_positions[sequence_num].append(position)\n",
    "    \n",
    "    sequence_list = []\n",
    "    \n",
    "    for sequence_num in selected_positions: \n",
    "        word_sequence, label_sequence = data_train[sequence_num]\n",
    "        selected_labels = [None for _ in range(len(word_sequence))]\n",
    "        \n",
    "        for position in selected_positions[sequence_num]:\n",
    "            selected_labels[position] = label_sequence[position]\n",
    "            __skip_extraction[sequence_num][position] = True\n",
    "        sequence_list.append([word_sequence, selected_labels])\n",
    "    \n",
    "    \n",
    "    # made some changes in the returning value \n",
    "    # prolly put something like \n",
    "    return sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f4d1b-031c-4fe7-863c-134c8d1930fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train function will required again \n",
    "def train(label_list, features_list):\n",
    "    \"\"\"Trains Minitagger on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "  \n",
    "    problem = liblinearutil.problem(label_list, features_list)\n",
    "    __liblinear_model = \\\n",
    "        liblinearutil.train(problem, liblinearutil.parameter(\"-q\"))\n",
    "      \n",
    "    quiet = False\n",
    "    if not quiet:\n",
    "        num_seconds = int(math.ceil(time.time() - start_time))\n",
    "        print(\"Training time: {0}\".format(\n",
    "            str(datetime.timedelta(seconds=num_seconds))))\n",
    "        if features_list is not None:\n",
    "            quiet_value = quiet\n",
    "            quiet = True\n",
    "            _, acc = predict(label_list, features_list)\n",
    "            quiet = quiet_value\n",
    "            print(\"Dev accuracy: {0:.3f}%\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f1f40-11f0-414a-ae08-29c6ca1281bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd71bf-22a6-4890-8a9d-aaf7faf185b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __train_silently(data_selected):\n",
    "    \"\"\"Trains on the argument data in silent mode.\"\"\"\n",
    "    self.__feature_extractor.is_training = True  # Reset for training.\n",
    "    quiet_value = self.quiet\n",
    "    self.quiet = True\n",
    "    self.train(data_selected, None)  # No need for development here.\n",
    "    self.quiet = quiet_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "bb39ab61-d64b-4a34-bbf6-e53ab0989e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['I', 'am', 'Winnie'], ['PRON', None, None]]]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__make_data_from_locations(data_train, [(0,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2011bc-bf70-4a11-87d5-41286b1846ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf27ac9-2ba9-429f-9b10-401474e432ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1228641-5000-41dd-a35b-d9951861821a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd18e2-0fb2-4cd1-8338-71cbc455afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actively(data_train, data_dev):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c19ea-cdd2-4844-a2f0-401090a4d983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be2204b-29b7-408d-897d-9ec3c6da9d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5dbef-c89f-4ec6-a4e4-cee25d8a7f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8ce8b-94a8-463e-bac2-61bee70775fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baabadd4-807c-425e-b552-025eae172911",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train_actively(self, data_train, data_dev):\n",
    "        \"\"\"Does margin-based active learning on the given data.\"\"\"\n",
    "\n",
    "    \n",
    "        \n",
    "        # Create an output directory.\n",
    "        if os.path.exists(self.active_output_path):\n",
    "            subprocess.check_output([\"rm\", \"-rf\", self.active_output_path])\n",
    "        os.makedirs(self.active_output_path)\n",
    "        logfile = open(os.path.join(self.active_output_path, \"log\"), \"w\")\n",
    "\n",
    "        def __make_data_from_locations(locations):\n",
    "            \"\"\"\n",
    "            Makes SequenceData out of a subset of data_train from given\n",
    "            location=(sequence_num, position) pairs.\n",
    "            \"\"\"\n",
    "            \n",
    "            selected_positions = collections.defaultdict(list)\n",
    "            for (sequence_num, position) in locations:\n",
    "                selected_positions[sequence_num].append(position)\n",
    "                \n",
    "\n",
    "            sequence_list = []\n",
    "            for sequence_num in selected_positions:\n",
    "\n",
    "                word_sequence, label_sequence = \\\n",
    "                    data_train.sequence_pairs[sequence_num]\n",
    "                \n",
    "                selected_labels = [None for _ in range(len(word_sequence))]\n",
    "          \n",
    "                for position in selected_positions[sequence_num]:\n",
    "                    selected_labels[position] = label_sequence[position]\n",
    "\n",
    "                    # This example will not be selected again.\n",
    "                    __skip_extraction[sequence_num][position] = True\n",
    "                sequence_list.append((word_sequence, selected_labels))\n",
    "             \n",
    "            # print(\"sequence_list: \", sequence_list)\n",
    "            \n",
    "            selected_data = SequenceData(sequence_list)\n",
    "            \n",
    "            # print(\"selected_data: \", selected_data)\n",
    "            return selected_data\n",
    "\n",
    "        def __train_silently(data_selected):\n",
    "            \"\"\"Trains on the argument data in silent mode.\"\"\"\n",
    "            self.__feature_extractor.is_training = True  # Reset for training.\n",
    "            quiet_value = self.quiet\n",
    "            self.quiet = True\n",
    "            self.train(data_selected, None)  # No need for development here.\n",
    "            self.quiet = quiet_value\n",
    "\n",
    "        def __interval_report(data_selected):\n",
    "            # Only report at each interval.\n",
    "            if data_selected.num_labeled_instances % \\\n",
    "                    self.active_output_interval != 0:\n",
    "                return\n",
    "#             print(\"------------------------\")\n",
    "#             print(\"now at __interval_report\")\n",
    "#             print(\"------------------------\")\n",
    "#             print(\"data_selected.num_labeled_instances\", data_selected.num_labeled_instances)\n",
    "#             print(\"self.active_output_interval\", self.active_output_interval)\n",
    "            \n",
    "            # Test on the development data if we have it.\n",
    "            if data_dev is not None:\n",
    "                quiet_value = self.quiet\n",
    "                self.quiet = True\n",
    "                _, acc = self.predict(data_dev)\n",
    "                self.quiet = quiet_value\n",
    "                message = \"{0} labels: {1:.3f}%\".format(\n",
    "                    data_selected.num_labeled_instances, acc)\n",
    "                print(message)\n",
    "                logfile.write(message + \"\\n\")\n",
    "                logfile.flush()\n",
    "\n",
    "            # Output the selected labeled examples so far.\n",
    "            file_name = os.path.join(\n",
    "                self.active_output_path,\n",
    "                \"example\" + str(data_selected.num_labeled_instances))\n",
    "            with open(file_name, \"w\") as outfile:\n",
    "                outfile.write(data_selected.__str__())\n",
    "\n",
    "        # Compute the (active_seed_size) most frequent word types in data_train.\n",
    "        sorted_wordcount_pairs = sorted(data_train.observation_count.items(),\n",
    "                                        key=lambda type_count: type_count[1],\n",
    "                                        reverse=True)\n",
    "        \n",
    "        # print(\"sorted_wordcount_pairs\")\n",
    "        # print(sorted_wordcount_pairs)\n",
    "        \n",
    "        seed_wordtypes = [wordtype for wordtype, _ in\n",
    "                          sorted_wordcount_pairs[:self.active_seed_size]]\n",
    "        \n",
    "        # print(\"seed_wordtypes\")\n",
    "        # print(seed_wordtypes)\n",
    "\n",
    "        # Select a random occurrence of each selected type for a seed example.\n",
    "        occurring_locations = collections.defaultdict(list)\n",
    "        \n",
    "        \n",
    "        for sequence_num, (observation_sequence, _) in \\\n",
    "                enumerate(data_train.sequence_pairs):\n",
    "            print(\"sequence_num: \",sequence_num)\n",
    "            print(\"observation_sequence: \", observation_sequence) # the words\n",
    "            print(\"_: \", _) # the tag\n",
    "            print(\"\")\n",
    "            \n",
    "            for position, word in enumerate(observation_sequence):\n",
    "                print(\"position: \", position)\n",
    "                print(\"word1: \", word)\n",
    "                \n",
    "                if word in seed_wordtypes:\n",
    "                    print(\"word2: \", word)\n",
    "                    occurring_locations[word].append((sequence_num, position))\n",
    "                print(\"\")\n",
    "                    \n",
    "        print(\"\")\n",
    "        print(\"occurring_locations: \", occurring_locations)\n",
    "        \n",
    "        \n",
    "        locations = [random.sample(occurring_locations[wordtype], 1)[0] for\n",
    "                     wordtype in seed_wordtypes]\n",
    "        \n",
    "        print(\"locations: \", locations)\n",
    "        \n",
    "        \n",
    "        data_selected = __make_data_from_locations(locations)\n",
    "        print(\"\\ndata_selected: \", data_selected)\n",
    "        \n",
    "        __train_silently(data_selected)  # Train for the first time.\n",
    "        __interval_report(data_selected)\n",
    "        \n",
    "        \n",
    "        print(\"testing on new case\")\n",
    "        print(\"------------------\")\n",
    "        print(len(locations))\n",
    "        print(data_train.num_labeled_instances)\n",
    "        print(\"------------------\")\n",
    "        \n",
    "        count = 0\n",
    "        while len(locations) < data_train.num_labeled_instances:\n",
    "            # Make predictions on the remaining (i.e., not on the skip list)\n",
    "            # labeled examples.\n",
    "            [label_list, features_list, location_list] = \\\n",
    "                self.__feature_extractor.extract_features(\\\n",
    "                data_train, False, __skip_extraction)\n",
    "\n",
    "            _, _, scores_list = \\\n",
    "                liblinearutil.predict(label_list, features_list,\n",
    "                                      self.__liblinear_model, \"-q\")\n",
    "            \n",
    "            # right here, I need to know about the score list being compute\n",
    "            # what is the score mean by here \n",
    "            # theory behind and later sort it \n",
    "            # choose it as a confidence pair or what??? \n",
    "            # since later train function is being called\n",
    "            # how it is being train this time? \n",
    "            \n",
    "            # Compute \"confidence\" of each prediction:\n",
    "            #   max_{y} score(x,y) - max_{y'!=argmax_{y} score(x,y)} score(x,y')\n",
    "            confidence_index_pairs = []\n",
    "            \n",
    "            for index, scores in enumerate(scores_list):\n",
    "                sorted_scores = sorted(scores, reverse=True)\n",
    "\n",
    "                # Handle the binary case: liblinear gives only 1 score whose\n",
    "                # sign indicates the class (+ versus -).\n",
    "                confidence = sorted_scores[0] - sorted_scores[1] \\\n",
    "                    if len(scores) > 1 else abs(scores[0])\n",
    "                confidence_index_pairs.append((confidence, index))\n",
    "            \n",
    "            count +=1 \n",
    "            print(\"count->\",count)\n",
    "            # Select least confident examples for next labeling.\n",
    "            print(\"before sorting: \",confidence_index_pairs)\n",
    "            confidence_index_pairs.sort()\n",
    "            \n",
    "            print(\"after sorting: \",confidence_index_pairs)\n",
    "            print(\"confidence_index_pairs[:self.active_step_size]:\", confidence_index_pairs[:self.active_step_size])\n",
    "            \n",
    "            for _, index in confidence_index_pairs[:self.active_step_size]:\n",
    "                print(_)\n",
    "                print(index)\n",
    "                print(\"location_list[index]\", location_list[index])\n",
    "                locations.append(location_list[index])\n",
    "            \n",
    "            print(\"locations: \", locations)\n",
    "            data_selected = __make_data_from_locations(locations)\n",
    "            \n",
    "            __train_silently(data_selected)  # Train from scratch.\n",
    "            __interval_report(data_selected)\n",
    "            \n",
    "            if count==1:\n",
    "                break\n",
    "\n",
    "        logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b572f-c6a8-4c2e-8cb2-e58e7c3e0698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d9c8f2-f811-4234-88bc-aca6b27995a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f6b4c-a0a7-4e7d-a914-e3204e6abe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb69c2b-29ab-4cb7-ae59-7a8cdcd563bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc977c0-7cc2-4f8c-9014-21c237d46268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c568d77-2f7f-4703-b454-2f8eca11475c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471a010-dbde-422b-a050-ecf67ea68029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527f85c-623f-4af2-8af2-2efa2b979b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31979dc-366c-4b15-b515-6bc0902f055d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5973a-f773-4a58-a9bc-759e47b9ee28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799dcbb-4f07-4fad-bb46-4ce4df7d35b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad9ba5-ecb6-4afe-bdcb-8db95c252b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff6c50-ffb2-45ec-90ad-56dcc9dea56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149605a-7570-4456-be81-afb133c6ebc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14607636-f1ef-4ba3-84b9-d8e60697d917",
   "metadata": {},
   "source": [
    "# feature extraction\n",
    "\n",
    "https://cetinsamet.medium.com/part-of-speech-pos-tagging-8af646a3d5bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "dcd72fe8-140b-4605-90cb-876da13f8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_features(observation_sequence, position):\n",
    "    if feature_template == \"baseline\":\n",
    "        raw_features = get_baseline_features(observation_sequence, position)\n",
    "    elif feature_template == \"embedding\":\n",
    "        assert __word_embedding is not None\n",
    "        raw_features = get_embedding_features(observation_sequence, position, __word_embedding)\n",
    "    elif feature_template == \"bitstring\":\n",
    "        assert __word_bitstring is not None\n",
    "        raw_features = get_bitstring_features(observation_sequence, position, __word_bitstring)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported feature template {0}\".format(feature_template))\n",
    "        \n",
    "    numeric_features = {}\n",
    "    for raw_feature in raw_features:\n",
    "        \n",
    "        if not raw_feature in __map_feature_str2num:\n",
    "            feature_number = len(__map_feature_str2num) + 1\n",
    "            __map_feature_str2num[raw_feature] = feature_number\n",
    "            __map_feature_num2str[feature_number] = raw_feature\n",
    "            \n",
    "        numeric_features[__map_feature_str2num[raw_feature]] = raw_features[raw_feature]\n",
    "        \n",
    "    return raw_features\n",
    "\n",
    "def extract_features(sequence_data, extract_all, skip_list): \n",
    "    label_list = []\n",
    "    features_list = []\n",
    "    location_list = []\n",
    "    \n",
    "    for sequence_num, (observation_sequence, label_sequence) in enumerate(sequence_data):\n",
    "        for position, label in enumerate(label_sequence):\n",
    "            \n",
    "            if skip_list and skip_list[sequence_num][position]:\n",
    "                continue \n",
    "                \n",
    "            if (not label is None) or extract_all:\n",
    "                label_list.append(__get_label(label))\n",
    "                features_list.append(__get_features(observation_sequence, position))\n",
    "                location_list.append((sequence_num, position))\n",
    "                \n",
    "    return label_list, features_list, location_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "e8a49ff4-e678-42c3-85b8-a5386e46b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_pairs = [[['Winnie', 'is', 'cute', 'the', 'cat'], ['PROPN', 'AUX', 'ADJ', 'DET', 'NOUN']],\n",
    "                  [['This', 'is', 'me'],['DET','AUX','PRON']]]\n",
    "# sequence_pairs = [[['Winnie', 'is', 'cute', 'the', 'cat'], ['PROPN', 'AUX', 'ADJ', 'DET', 'NOUN']]]\n",
    "# sequence_pairs = [[['Winnie', 'is', 'cute', 'the', 'cat'], ['PROPN',None,  'ADJ', 'DET', 'NOUN']]]\n",
    "# data_train = [[['The', 'dog', 'saw', 'the', 'cat'], ['D', 'N', 'V', 'D', 'N']]]\n",
    "# sequence_pairs = [[['The', 'dog', 'saw', 'the', 'cat'], ['D', 'N', 'V', 'D', 'N']]]\n",
    "\n",
    "label_list, features_list, _ = extract_features(sequence_pairs, False, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6e3647eb-666c-44eb-a440-85d6a9a7ff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 11, 4, 2, 3, 2, 11, 12]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3ef1b41d-c8d6-4d63-a518-ea90ae33b085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word(0)=Winnie': 1,\n",
       "  'is_capitalized(0)=True': 1,\n",
       "  'prefix1(0)=W': 1,\n",
       "  'suffix1(0)=e': 1,\n",
       "  'prefix2(0)=Wi': 1,\n",
       "  'suffix2(0)=ie': 1,\n",
       "  'prefix3(0)=Win': 1,\n",
       "  'suffix3(0)=nie': 1,\n",
       "  'prefix4(0)=Winn': 1,\n",
       "  'suffix4(0)=nnie': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=_START_': 1,\n",
       "  'word(-2)=_START_': 1,\n",
       "  'word(+1)=is': 1,\n",
       "  'word(+2)=cute': 1},\n",
       " {'word(0)=is': 1,\n",
       "  'is_capitalized(0)=False': 1,\n",
       "  'prefix1(0)=i': 1,\n",
       "  'suffix1(0)=s': 1,\n",
       "  'prefix2(0)=is': 1,\n",
       "  'suffix2(0)=is': 1,\n",
       "  'prefix3(0)=is*': 1,\n",
       "  'suffix3(0)=*is': 1,\n",
       "  'prefix4(0)=is**': 1,\n",
       "  'suffix4(0)=**is': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=Winnie': 1,\n",
       "  'word(-2)=_START_': 1,\n",
       "  'word(+1)=cute': 1,\n",
       "  'word(+2)=the': 1},\n",
       " {'word(0)=cute': 1,\n",
       "  'is_capitalized(0)=False': 1,\n",
       "  'prefix1(0)=c': 1,\n",
       "  'suffix1(0)=e': 1,\n",
       "  'prefix2(0)=cu': 1,\n",
       "  'suffix2(0)=te': 1,\n",
       "  'prefix3(0)=cut': 1,\n",
       "  'suffix3(0)=ute': 1,\n",
       "  'prefix4(0)=cute': 1,\n",
       "  'suffix4(0)=cute': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=is': 1,\n",
       "  'word(-2)=Winnie': 1,\n",
       "  'word(+1)=the': 1,\n",
       "  'word(+2)=cat': 1},\n",
       " {'word(0)=the': 1,\n",
       "  'is_capitalized(0)=False': 1,\n",
       "  'prefix1(0)=t': 1,\n",
       "  'suffix1(0)=e': 1,\n",
       "  'prefix2(0)=th': 1,\n",
       "  'suffix2(0)=he': 1,\n",
       "  'prefix3(0)=the': 1,\n",
       "  'suffix3(0)=the': 1,\n",
       "  'prefix4(0)=the*': 1,\n",
       "  'suffix4(0)=*the': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=cute': 1,\n",
       "  'word(-2)=is': 1,\n",
       "  'word(+1)=cat': 1,\n",
       "  'word(+2)=_END_': 1},\n",
       " {'word(0)=cat': 1,\n",
       "  'is_capitalized(0)=False': 1,\n",
       "  'prefix1(0)=c': 1,\n",
       "  'suffix1(0)=t': 1,\n",
       "  'prefix2(0)=ca': 1,\n",
       "  'suffix2(0)=at': 1,\n",
       "  'prefix3(0)=cat': 1,\n",
       "  'suffix3(0)=cat': 1,\n",
       "  'prefix4(0)=cat*': 1,\n",
       "  'suffix4(0)=*cat': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=the': 1,\n",
       "  'word(-2)=cute': 1,\n",
       "  'word(+1)=_END_': 1,\n",
       "  'word(+2)=_END_': 1},\n",
       " {'word(0)=This': 1,\n",
       "  'is_capitalized(0)=True': 1,\n",
       "  'prefix1(0)=T': 1,\n",
       "  'suffix1(0)=s': 1,\n",
       "  'prefix2(0)=Th': 1,\n",
       "  'suffix2(0)=is': 1,\n",
       "  'prefix3(0)=Thi': 1,\n",
       "  'suffix3(0)=his': 1,\n",
       "  'prefix4(0)=This': 1,\n",
       "  'suffix4(0)=This': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=_START_': 1,\n",
       "  'word(-2)=_START_': 1,\n",
       "  'word(+1)=is': 1,\n",
       "  'word(+2)=me': 1},\n",
       " {'word(0)=is': 1,\n",
       "  'is_capitalized(0)=False': 1,\n",
       "  'prefix1(0)=i': 1,\n",
       "  'suffix1(0)=s': 1,\n",
       "  'prefix2(0)=is': 1,\n",
       "  'suffix2(0)=is': 1,\n",
       "  'prefix3(0)=is*': 1,\n",
       "  'suffix3(0)=*is': 1,\n",
       "  'prefix4(0)=is**': 1,\n",
       "  'suffix4(0)=**is': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=This': 1,\n",
       "  'word(-2)=_START_': 1,\n",
       "  'word(+1)=me': 1,\n",
       "  'word(+2)=_END_': 1},\n",
       " {'word(0)=me': 1,\n",
       "  'is_capitalized(0)=False': 1,\n",
       "  'prefix1(0)=m': 1,\n",
       "  'suffix1(0)=e': 1,\n",
       "  'prefix2(0)=me': 1,\n",
       "  'suffix2(0)=me': 1,\n",
       "  'prefix3(0)=me*': 1,\n",
       "  'suffix3(0)=*me': 1,\n",
       "  'prefix4(0)=me**': 1,\n",
       "  'suffix4(0)=**me': 1,\n",
       "  'is_all_nonalphanumeric(0)=False': 1,\n",
       "  'is_float(0)=False': 1,\n",
       "  'word(-1)=is': 1,\n",
       "  'word(-2)=This': 1,\n",
       "  'word(+1)=_END_': 1,\n",
       "  'word(+2)=_END_': 1}]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89de4bb-5967-4045-9e75-0842c6a11c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "vectorizer = DictVectorizer()                               # Initialize dictionary vectorizer\n",
    "model      = LogisticRegressionCV()                         # Initialize logistic regression model\n",
    "\n",
    "vectorizer.fit(features)                                    # Train vectorizer\n",
    "vectorized_features = vectorizer.transform(features)        # Convert features from dictionary form to vector form\n",
    "model.fit(vectorized_features, pos_labels)                  # Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458d12e-5ce9-4850-8048-463b9383a97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa830b75-4552-43a2-bad9-cd0f4ab7f964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melex-kernel",
   "language": "python",
   "name": "melex-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
